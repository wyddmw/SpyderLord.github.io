<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>Optimization | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/Optimization" title="Optimization">Optimization</a></h1>
        <p class="entry-date">2018-04-19</p>
        <h2 id="optimization-and-parameter-update">Optimization and Parameter Update</h2>

<h3 id="损失函数重新复习">损失函数重新复习</h3>
<p>　　损失函数的作用是用来进行量化某个具体权重集W的质量，我们现在使用到的损失函数主要是softmax函数，通过交叉熵来评估模型的好坏与否。复习一下softmax函数的用法，每一次看之前看过的内容都会有新的收获。先给出wiki上关于softmax函数的百科链接<a href="https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0">softmax</a>。在数学，尤其是概率论和相关理论中，softmax函数或是称为归一化指数函数，是逻辑函数的一种推广。
<img src="/downloads/softmax.png" alt="" />
　　softmax函数实际上是有限项离散概率分布的梯度的对数的归一化，放在我们的分类问题中，我们可以认为得到的是某一个输入在多个类别中对应的分类。得到了概率值之后再进行熵的计算，简单来说就是直接计算上述求得的概率值的自然对数值。<br />
　　想清楚了这点，我觉得其实当我们再看到loss的时候就可以想出来对应的分类的概率是多少。如果我们的正确的概率是0.5，计算得到的loss的结果就应该是0.7左右，所以除非是svm形式的损失函数，softmax计算出来的损失值不会特别大，如果特别大说明对应的概率是非常小的。</p>
<h4 id="所以再次印证了我现在越来越重视的一个观点理论基础一定要打扎实了遇到了问题将问题和理论结合起来而不是凭空去猜想">所以再次印证了我现在越来越重视的一个观点，理论基础一定要打扎实了，遇到了问题将问题和理论结合起来，而不是凭空去猜想。</h4>
<h3 id="复习optimization的流程">复习optimization的流程</h3>
<p>　　因为在tensorflow上没有反向传播的过程，还是以之前的softmax为示例重新去回顾一下整个optimization的过程是什么样的。</p>
<ul>
  <li></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
  <span class="s">"""
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  """</span>
  <span class="c"># Initialize the loss and gradient to zero.</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
  <span class="n">num_train</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">num_class</span><span class="o">=</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">index_tuple</span><span class="o">=</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_class</span><span class="p">),</span><span class="nb">list</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
      <span class="n">scores</span><span class="o">=</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
      <span class="n">scores_shifted</span><span class="o">=</span><span class="n">scores</span><span class="o">-</span><span class="n">scores</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>   <span class="c">#shift the value inside the vector</span>
                                           <span class="c">#to make sure the highest value is zero </span>
                                           <span class="c">#in order to prevent the data blowup</span>
      <span class="n">loss_i</span><span class="o">=-</span><span class="n">scores_shifted</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores_shifted</span><span class="p">)))</span>
      <span class="n">loss</span><span class="o">+=</span><span class="n">loss_i</span>
      <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_class</span><span class="p">):</span>
          <span class="n">softmax_score</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores_shifted</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores_shifted</span><span class="p">))</span>
          <span class="k">if</span> <span class="n">j</span><span class="o">==</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
              <span class="n">dW</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">+=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">+</span><span class="n">softmax_score</span><span class="p">)</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">dW</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span><span class="o">+=</span><span class="n">softmax_score</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="n">loss</span><span class="o">/=</span><span class="n">num_train</span>
  <span class="n">loss</span><span class="o">+=</span><span class="n">reg</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
  <span class="n">dW</span><span class="o">=</span><span class="n">dW</span><span class="o">/</span><span class="n">num_train</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">reg</span><span class="o">*</span><span class="n">W</span>
  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div>
<ul>
  <li>从代码中可以看到上面这种代码在最优化的过程中整个训练样本的数据都参与到计算中的，每一次在计算导数的时候，整个数据集中的每一个数据都会参与到计算中。
    <h3 id="梯度下降gradient-descent">梯度下降——Gradient Descent</h3>
    <p>　　当我们可以计算损失函数的梯度的时候，程序会重复地计算梯度然后对参数进行更新，这个过程称为梯度下降，核心思想就是我们一直跟着梯度走，直到结果不再变化。</p>
  </li>
  <li>Mini-Batch gradient descent<br />
　　斯坦福的cs231n的notes中如下写道
<img src="/downloads/mini.png" alt="" />
　　什么意思呢，就是在现实问题中，我们会遇到训练样本非常庞大的问题，可能会达到上百万的级别。如果我们按照上面代码中使用的方法进行optimization的话，每次计算整个数据集上的损失函数值，然后进行一次参数的优化，这样做是非常浪费的，效率很低。</li>
  <li>对比120万张图片的数据损失的均值和只计算1000张子集的数据损失的均值的时候，结果应该是一样的。实际情况中，数据集肯定不会包含重复的图像，那么小批量数据的梯度就是对整体数据集梯度的一个近似。因此，在实践中，通过计算小批量数据的梯度，可以实现更加快速的收敛，并以此来进行更频繁的参数更新。</li>
  <li>在小批量数据策略中，有一个极端的情况，就是每一个批量中只有一个样本，这种策略叫做随机梯度下降(Stochastic Gradient Descent–SGD)。这种策略在实际中很少用到。虽然实际上是指代用一个数据来计算梯度，我们有时候还是会用SGD来代指小批量数据梯度下降。
    <h3 id="知乎中关于sgd的一些解答">知乎中关于SGD的一些解答</h3>
  </li>
  <li>随机梯度下降：在每次更新的时候使用一个样本，随机也就是说我用样本中的一个例子来近似我们所有的样本，对于最优化问题，凸问题，虽然不是每一次迭代得到的损失函数都向着全局最优化的方向，但是大的整体的方向是向着全局最优解的．</li>
  <li>批量梯度下降：在每次更新的时候使用全部的样本，在更新的时候，所有的样本都有贡献，计算出来一个标准梯度。</li>
  <li>mini-batch梯度下降：在每次更新的时候，我们使用b个样本，使用一些较小的样本来近似代替全部的。在深度学习中，这种方法是使用最多的。</li>
</ul>

    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Parameter_Update">Parameter_Update</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
