<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>大数据和机器学习学习笔记 | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="大数据和机器学习学习笔记">大数据和机器学习学习笔记</a></h1>
        <p class="entry-date">2018-06-26</p>
        <h2 id="section">补充一些相关的概念</h2>
<ul>
  <li>古典概型：也叫传统概率，如果一个随机实验所包含的单位事件是有限的，且每个单位事件发生的可能性均相等，那么这个随机实验就叫做拉普拉斯试验，这种条件下的概率模型就叫做古典概型。判断一个试验是否为古典概型，在于要判断这个试验是否具有古典概型的两个特征——有限性和等可能性，只有同时具备这两个特点的概型才是古典概型。</li>
  <li>几何概型：一种概率模型，在这个模型下，随机实验所有可能的结果是无限的，并且每一个基本结果发生的概率是相同的。比如一个人到单位的时间在某个时间区间内等。判断一个试验是否为几何概型在于这个试验是否具有几何概型的两个特征——无限性，等可能性。几何概型和古典概型是相对的。</li>
  <li>概率这个概念：概率这个定义是通过统计得到的。在抛硬币的场景中，我们说的概率实际上是通过统计得出的，而并非是因为硬币存在两个面，概率本身是对大量样本分布比例的一种解释，而不是对单一一次事件的可能性的解释。</li>
</ul>

<h2 id="section-1">信息论相关</h2>
<p>　　信息最为广泛接受的一种定义是——信息是倍消除的不确定性。
- 信息量：在信息论中，对信息量是有确定解释并且是可以量化计算的，这里提到的信息量是一种信息数量化度量的规则。具体的补充待看了通信原理之后再进行更加详细的补充。
- 我们在计算信息量的时候，使用的概率是先验概率。
- 香农公式：在通信工程中，存在一个非常重要的公式就是香农公式。C=Blog2(1+S/N)。其中B表示码元速率，S/N表示信噪比，传输的信号功率和在这个信道中产生的各种信号噪声的功率的大小比值。这个公式说明了信号传输速率和信噪比以及带宽之间的关系。从公式来看，带宽越大传输的速率就越大。
- 信息熵：它可以认为是用来描述信息的杂乱程度的。看完之后，我认为信息熵这个概念是用来对整个事件进行描述的，该事件发生可能会有多种不同的情况，每一种情况对应的会有一个信息量，而信息熵就是对各个不同的情况的信息量的数学期望，对整个事件进行一个整体上的描述。</p>

<h2 id="section-2">多维向量空间</h2>
<ul>
  <li>向量的概念：具有大小和方向的几何对象。</li>
  <li>维度：维度指的是参照系，有多少个维度就有多少个参照系，维度的设定一般具有正交性的。</li>
</ul>

<h2 id="section-3">分割线，从下面开始，就要开始接触相关的机器学习的相关内容了。</h2>

<h2 id="section-4">回归分析</h2>
<ul>
  <li>线性回归：线性回归是利用数理统计学中的回归分析来确定两种或两种以上变量之间相互依赖的定量关系的一种统计分析方法。重点在于——线性回归是用来确定一种定量关系的，这是重点所在。</li>
  <li>线性回归中使用的是残差分析，残差在数理统计中是指实际观察值和估计值（拟合值）之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话，我们可以将残差看做误差的一个观测值。在残差网络中，我们将中间的hidden layer的功能看做是残差，通过网络的优化，将这个残差不断缩小，进而可以实现identity match的效果。回到我们的回归分析中来。在线性回归中，我们希望最终能够得到一个函数来表示y和x之间的关系。一种非常经典的用来进行线性回归中的系数猜测的方法是——最小二乘法，我们希望得到的这样的一个函数关系，预测值和实际值之间的差值最小。我们假设有多个x、y的样本值，我们尝试用一个函数来进行拟合这种关系，残差的表示就是e=ax+b+-y，然后对e计算绝对值。误差的大小是预测值ax+b和观测值y之间的差值。我们尝试讲所有的差值进行求和，构造出来一个函数，下面的问题就转换为如何将这个残差函数值最小化。写到这里就可以和梯度下降结合起来了，残差和的函数分别对a和b进行求导，然后对a和b分别进行调整，最终使得残差的和最小，实现理想的拟合过程，通过梯度下降的方式对a和b的值进行调整。写到这里，想起来之前和研究生讨论过的一个问题，他去听讲座的时候，主讲人讲的是目前常用的训练的优化的方法或是说损失函数本质上都是最小二乘法。现在仔细想想确实是这样的，通过最小二乘法进行优化，通过最小化误差的平方和寻找数据的最佳函数匹配。</li>
</ul>

<h2 id="section-5">分类问题</h2>
<p>　　在之前专门写的一个博客中有写到，分类问题的输出结果是一个离散的值，和回归问题给出的是一个连续的值是不同的。分类算法一类大的算法，都是用来解决这种离散变量预测值的。</p>

<h3 id="section-6">贝叶斯分析</h3>
<p>　　朴素贝叶斯：其实之前在看西瓜书或是看其他相关机器学习算法的时候，就有看到过介绍过贝叶斯分析的。贝叶斯分析是非常牛逼的算法，之前学习概率论的时候有简单学习过贝叶斯分析，在没有看书的前提下回想这个贝叶斯分析，我所能想起来的就是这个算法是利用先验概率和概率密度去推算后验概率密度的。
- 贝叶斯决策理论方法：1.已知类条件概率密度参数表达式和先验概率。2.利用贝叶斯公式转换成后验概率。3.根据后验概率大小进行决策分类。<br />
　　简单来说，朴素贝叶斯公式就是利用统计中的“条件概率”来进行分类的一种算法。贝叶斯概率研究的是条件概率，也就是研究的场景是在带有某些场景的前提条件下，或者在某些背景条件的约束下发生的概率问题。
- 贝叶斯公式：P(A∩B) = P(A)×P(B｜A)=P(B)×(A｜B)。贝叶斯公式一般简写为：P(A｜B)×P(B)=P(B｜A)×P(A)。对应变量的意义如下:
- P(A):事件A的先验概率，就是一般情况下事件A发生的概率。P(B｜A)的叫做似然概率，是A假设条件成立的情况下发生B的概率。P(A｜B)叫做后验概率，在事件B发生的情况下发生A的概率，也就是要计算的概率。P(B)叫做标准化常量，和A的先验概率是定义类似，就是一般情况下，事件B发生的概率。所以使用贝叶斯分析的思路就是使用现有的数据进行建模处理，可能我需要结合代码才能更加明确这里所说的建模是什么样的一个概念或是操作。从目前自己的理解来看就是使用现有的数据集去计算相关的各个概率值，然后根据计算出来的各个先验概率以及相关的似然概率等去得到最后我们想要的后验概率。</p>

<h3 id="svm">SVM支持向量机</h3>
<p>　　摘抄之前写的博客中的一点内容，作为一点回顾。SVM本身对应的是一个二分类的问题，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略是间隔最大化，最终可以转换成为一个凸二次规划问题的求解。一个线性分类器的学习目标就是在n维的数据空间中找到一个超平面，这个超平面的方程表示为W(T)X+b=0。距离超平面最近的几个训练样本点使符号函数g(z)=sign(x)成立，这些样本点称为是支持向量。然后我们需要做的是异类支持向量到超平面的距离最大化——可以和SVM的损失函数结合起来。
- 关于超平面的一点说明：超平面是一个抽象的概念，在一维空间中就是一个点，用x+A=0来表示；在二维空间中就是一条线，用Ax+By+c=0来表示，Ax+By+c&gt;0表示的是一类，&gt;0的表示为一类；在三维空间中就是一个面，用Ax+By+Cz+d=0来表示等。
- 距离的问题：在回归问题中，我们希望拟合出来一条曲线能够和得到的样本数据点的距离最近，在SVM支持向量机中，我们希望让被分开的两个类别的距离尽可能远，也就是相差越远分的越开。在平面直角坐标系中点到直线的距离表示为d=｜Ax+By+c｜/sqrt(A^2+B^2)。超平面的表达式实际上可以简写成g(v)=wv+b。其中v是样本向量，b是常数。在二维的空间中，v就是(x,y)，三维空间中就是(x,y,z)，其余的情况是类似的，而w也是一个向量，在二维空间中就是(A,B)，在三维空间中就是(A,B,C)。
- 有点不太想继续写下去了，做一个简单的小结：SVM解决问题的方法描述起来对应的是下面的步骤：
1. 将所有的样本和其对应的分类标记交给算法进行训练。
2. 如果线性可分，就直接找出超平面
3. 如果线性不可分，就将数据映射到n+1高维空间，找出超平面
4. 最后得到超平面的表达式，也就是分类的函数。</p>

<h2 id="section-7">分割线2018-06-02</h2>
<p>　　在看知乎上一个关于SVM属于神经网络范畴吗的提问中，看到了贾扬清给出的一个回答，整理一下放在这里：<br />
　　具体地说：线性SVM的计算部分和一个单层神经网络一样，就是一个矩阵乘积。SVM的关键在于它的hinge loss以及maximum margin的想法。对于处理非线性数据，SVM和神经网络走的是两条不同的道路：神经网络通过多个隐层的方法来实现非线性的函数，可解释性不高。SVM则采用kernel核函数的方法，核函数在理论上比较完备。<br />
　　看到现在，感觉自己一直有点陷在一个误区中，就是我一直在尝试在将神经网络和支持向量机强行结合起来，感觉没有这个必要啊。重新更换一下思路，我们使用神经网络就是为了提取特征，然后在提取到的特征后面添加一个线性分类的支持向量机，对分类评价的考量就是使用SVM的损失函数——hinge loss结合margin。这样的话重新理解就比较好理解了。然后重新回来看，如果我们能够对输入直接进行线性分类，那就说明在当前的空间内是线性可分的，我们能够找到一个超平面，需要注意的是，我们在这里说的当前维度是已经经过神经网络处理之后的特征，对于最原始的生数据我们可以直接找到线性分类的超平面么？讲道理可能是找不到的，或者说就算是找到了但是最后的分类的效果也不会非常理想。如果是完全的SVM的想法，就会对输入的原始生数据使用kernel映射到高维空间中，然后再尝试找一个超平面进行分类。所以看起来，神经网络应该也算是起到了对数据的处理的作用，按照上面的说法，就是对数据的非线性处理。实际上SVM和神经网络对非线性数据的处理是两种不同的方法，神经网络是通过多层不同的隐层来实现非线性的函数，但是SVM是通过核函数来实现的。</p>

<h2 id="section-8">分割线 2018-07-03</h2>
<p>　　在看notes的时候，看到了对回归问题的损失函数的介绍，本质上就是最小二乘法的思路，使用L2范式，不断拟合预测值和实际值，缩小它们之间的差距。
<img src="/downloads/回归损失.png" alt="" height="130" /></p>

    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/FLaME%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3">FlaME论文阅读理解</a></li>
        
            <li><a href="/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E7%9A%84%E8%BF%90%E5%8A%A8">刚体运动，视觉SLAM的观测方程</a></li>
        
            <li><a href="/%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%9A%84%E7%A8%8B%E5%BA%8F">加载预训练好的模型进行预测</a></li>
        
            <li><a href="/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95">SSD中的一些重要的算法</a></li>
        
            <li><a href="/SSD">SSD-MobileNet</a></li>
        
            <li><a href="/SVS%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86">SVS算法整理</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0(%E4%B8%89)">DeepLearning复习(三)</a></li>
        
            <li><a href="/Jetson_TX2%E5%BC%80%E5%8F%91">Jetson_TX2开发</a></li>
        
            <li><a href="/C++(%E4%B8%80)">C++复习（一）</a></li>
        
            <li><a href="/%E5%9C%A8linux%E4%B8%8A%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAC++%E7%9A%84%E7%A8%8B%E5%BA%8F">在linux上使用Vim编写一个C/C++程序</a></li>
        
            <li><a href="/Ubuntu16.04%E4%B8%8B%E5%AE%89%E8%A3%85cuda-cudnn">ubuntu16.04安装显卡驱动及cuda</a></li>
        
            <li><a href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1">研究生的数学建模</a></li>
        
            <li><a href="/Faster_RCNN">Faster RCNN</a></li>
        
            <li><a href="/Fast-RCNN">Fast R-CNN</a></li>
        
            <li><a href="/R-CNN">R-CNN学习笔记</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0%E4%BA%94">DeepLearning复习(五)</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0%E5%9B%9B">DeepLearning(四)</a></li>
        
            <li><a href="/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%A4%8D%E4%B9%A0">Deeplearning复习(三)</a></li>
        
            <li><a href="/%E4%B8%80%E6%AE%B5%E5%A5%94%E6%B3%A2%E4%B9%8B%E5%90%8E">一段奔波结束</a></li>
        
            <li><a href="/%E9%99%88%E8%80%81%E5%B8%88%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93">陈老师的论文简述</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0-%E4%BA%8C">DeepLearning复习(二)</a></li>
        
            <li><a href="/%E5%8F%88%E6%98%AF%E4%B8%80%E7%AF%87%E5%85%B3%E4%BA%8E%E4%BF%9D%E7%A0%94%E7%9A%84%E9%9A%8F%E6%84%9F">又是保送过程中的一点随感</a></li>
        
            <li><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">大数据和机器学习学习笔记</a></li>
        
            <li><a href="/%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%AB%AF%E5%8D%88%E5%87%BA%E8%A1%8C">一个人的端午出游</a></li>
        
            <li><a href="/%E8%BF%98%E6%98%AF%E5%8C%97%E4%BA%AC">还是北京</a></li>
        
            <li><a href="/FaceNet%E5%AD%A6%E4%B9%A0">FaceNet学习</a></li>
        
            <li><a href="/%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AF%94%E8%BE%83">分类和回归之间的比较</a></li>
        
            <li><a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0">神经网络的复习</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0-%E4%B8%80">DeepLearning复习（一）</a></li>
        
            <li><a href="/BinarySearchTree">Binary Search Tree</a></li>
        
            <li><a href="/%E5%B0%86%E6%9D%A5%E7%9A%84%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%84%9F%E8%B0%A2%E7%8E%B0%E5%9C%A8%E5%8A%AA%E5%8A%9B%E7%9A%84%E8%87%AA%E5%B7%B1">将来的你一定会感谢现在努力的自己</a></li>
        
            <li><a href="/PSPNet%E7%9A%84%E5%AD%A6%E4%B9%A0">PSP网络的学习</a></li>
        
            <li><a href="/ResNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0">ResNet网络的学习</a></li>
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91SLAM">激光雷达SLAM</a></li>
        
            <li><a href="/%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E9%87%8D%E5%91%BD%E5%90%8D">对文件进行重命名</a></li>
        
            <li><a href="/opencv%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E8%84%9A%E6%9C%AC">opencv的一个小小脚本</a></li>
        
            <li><a href="/%E4%B8%80%E4%B8%AAidea">目标检测和深度估计的结合</a></li>
        
            <li><a href="/object_detection_api">使用tensorflow的API直接进行训练</a></li>
        
            <li><a href="/tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96">TensorBoard可视化查看模型的graph</a></li>
        
            <li><a href="/scope">arg_scope</a></li>
        
            <li><a href="/tensorflow%E7%9A%84session">tensorflow的session</a></li>
        
            <li><a href="/protobuf">protobuf</a></li>
        
            <li><a href="/stringstream">stringstream</a></li>
        
            <li><a href="/C++%E4%B8%AD%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%AC%A6">C++中的几个操作符</a></li>
        
            <li><a href="/pip3_error">pip3更新之后出错</a></li>
        
            <li><a href="/%E7%94%A8AD%E7%94%BBPCB%E6%9D%BF">使用AD制作PCB板</a></li>
        
            <li><a href="/cuda%E5%92%8Ccudnn">什么是cuda和cuDNN</a></li>
        
            <li><a href="/%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85ubuntu16.04.05">ubuntu16.04.5的安装</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E7%95%8C%E8%AF%BB%E5%8F%96%E5%8F%82%E6%95%B0">python解析命令行参数</a></li>
        
            <li><a href="/anaconda%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85">anaconda安装第三方包</a></li>
        
            <li><a href="/%E5%8D%B7%E7%A7%AF%E5%92%8C%E6%BB%A4%E6%B3%A2%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB">卷积和滤波之间的区别</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98">参数初始化的问题</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98">参数和超参数的问题</a></li>
        
            <li><a href="/%E6%9D%82%E8%AE%B0%E9%9A%8F%E7%AC%94">杂记随笔</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E8%B4%AB%E5%AF%8C%E5%B7%AE%E8%B7%9D">关于贫富差距的感想</a></li>
        
            <li><a href="/%E4%BE%9D%E7%84%B6%E5%9B%9E%E5%BD%92%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E5%B7%AE%E5%88%AB%E7%9A%84%E9%97%AE%E9%A2%98">依然是SVM和线性回归之间的对比问题</a></li>
        
            <li><a href="/%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8">训练集验证集和测试集的作用</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%89">C语言复习三</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%AF%BC%E5%85%A5%E5%A4%B4%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98">关于导入头文件的问题</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%BA%8C">C语言复习二</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%80">C语言复习一</a></li>
        
            <li><a href="/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E8%BD%BD">加载预训练好的模型时遇到的问题</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E9%83%A8%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0">Python从外部传入参数</a></li>
        
            <li><a href="/%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B">如何保存一个训练好的模型或是参数</a></li>
        
            <li><a href="/CV%E6%8C%91%E6%88%98%E8%B5%9B">CV中非常重要的挑战赛</a></li>
        
            <li><a href="/%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95">图论算法</a></li>
        
            <li><a href="/%E8%BF%91%E4%B8%A4%E5%A4%A9%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%84%9F%E6%83%B3">近两天对深度学习的一点感想</a></li>
        
            <li><a href="/benchmark">几个概念的理解</a></li>
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
            <li><a href="/%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93">通信原理复习总结</a></li>
        
            <li><a href="/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1">课程设计</a></li>
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
