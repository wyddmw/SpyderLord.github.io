<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>Batch_Normalization | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/Batch_Normalization" title="Batch_Normalization">Batch_Normalization</a></h1>
        <p class="entry-date">2018-04-14</p>
        <h2 id="batch-normalization的一些备注">Batch Normalization的一些备注</h2>

<p>　　BN算法是用来解决 Internal Covariate Shift问题的，那么首先我们需要理解什么是Internal Covariate Shift。Batch Normalization是基于Mini-Batch SGD的结构的。对于深度学习这种包含很多隐层的网络结构，在训练的过程中，因为各层参数一直在发生变化，所以每个隐层都面临covariate shift的问题，也就是在训练的过程中，隐层的输入分布不断在变化。这就是所谓的Internal Covariate Shift问题，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是只是发生在输入层。<br />
　　如果输入值X的分布一直在变化，那我们就没有办法稳定的学习规律。然后就提出了BatchNorm的基本思想，能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了Internal Covariate Shift问题了。BN并不是凭空产生的，是有启发来源的，之前的研究中，发现，如果在图像处理中对输入图像进行白化操作的话————就是对输入数据分布变换到0均值，单位方差的正态分布————那么神经网络就会较快速的收敛————对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思就是深度神经网络的每一个隐层都是输入层，不过是对于下一层来说的，那么我们能不能对每个隐层都做白化呢？可以理解为对深层神经网络每个神经元的激活值做简化版的White处理。<br />
　　BN算法可以提高学习的收敛速度。对于BN算法可以参考这个链接：<a href="http://blog.csdn.net/whitesilence/article/details/75667002">http://blog.csdn.net/whitesilence/article/details/75667002</a>
<a href="http://blog.csdn.net/whitesilence/article/details/75667002">http://blog.csdn.net/oppo62258801/article/details/76405196</a><br />
　　BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差1的正态分布通过平移压缩或是扩大曲线尖锐程度，调整为均值为0方差为1的标准正态分布。调整到正态分布的作用是什么：意味着在一个标准差的范围内，有64%的概率X的之落在[-1,1]的范围内，两个标准差的范围内，也就是95%的概率X的值落在[-2,2]的范围内。<br />
　　激活值x=WU+B，U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid函数，原始没有经过调整的x的正态分布是均值为-6,方差是-1的，对应的95%的概率落在[-8,-4]之间，对应的非线性的函数值就会明显接近0,这就是典型的梯度饱和区。但是经过BN处理之后，95%的概率落在一段非线性函数接近现行变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也就是梯度变化比较大，也就是梯度非饱和区。所以BN的操作就是把因曾神经元激活输入从不同的正态分布拉回到均值为零，方差为1的正态分布，这样就使得大部分的Activition的之落入到非线性函数的线性区域，其对应的导数远离导数饱和区，这样来加快训练收敛的速度。
在scale和shift操作中，就是相当于对标准正态分布左移或是右移、长胖或是变瘦一点，应该是为了找到非线性和线性的一个平衡点？</p>


    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0">DeepLearning复习</a></li>
        
            <li><a href="/BinarySearchTree">Binary Search Tree</a></li>
        
            <li><a href="/%E5%B0%86%E6%9D%A5%E7%9A%84%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%84%9F%E8%B0%A2%E7%8E%B0%E5%9C%A8%E5%8A%AA%E5%8A%9B%E7%9A%84%E8%87%AA%E5%B7%B1">将来的你一定会感谢现在努力的自己</a></li>
        
            <li><a href="/PSPNet%E7%9A%84%E5%AD%A6%E4%B9%A0">PSP网络的学习</a></li>
        
            <li><a href="/ResNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0">ResNet网络的学习</a></li>
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/CV%E6%8C%91%E6%88%98%E8%B5%9B">CV中非常重要的挑战赛</a></li>
        
            <li><a href="/%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95">图论算法</a></li>
        
            <li><a href="/%E8%BF%91%E4%B8%A4%E5%A4%A9%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%84%9F%E6%83%B3">近两天对深度学习的一点感想</a></li>
        
            <li><a href="/benchmark">几个概念的理解</a></li>
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
