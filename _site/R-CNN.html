<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>R-CNN学习笔记 | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/R-CNN" title="R-CNN学习笔记">R-CNN学习笔记</a></h1>
        <p class="entry-date">2018-08-22</p>
        <h2 id="section">前言</h2>
<p>　　在学习CS231n的时候就看到过这个文章，但是R-CNN主要解决的是目标检测的任务，所以在这方面看的内容非常浅，王老师发来了6篇paper基本上都是建立在R-CNN的基础上的，开始学习Detection这方面的任务。所有的paper都是建立在R-CNN的基础上的，所以需要重新看一下这个RCNN网络。</p>

<h2 id="semantic-segmentationinstance-segmentation">semantic segmentation和instance segmentation之间的区别</h2>
<p>　　 关于instance segmentation这个概念也是在和同济大学的老师在聊天的时候他告诉我的。当时还没有什么感觉，回到学校看论文的时候又出现了这个概念，有问题上知乎啊：semantic segmentation和instance segmentation之间的区别。
- Semantic segmentation的目的是在一张图里分割聚类出不同物体的pixel，是一个pixel-wise prediciton的预测。Semantic Segmentation将图片里面物体所在的区域分割出来了，但是并没有告诉我们区域里面有多少个人，以及每个人所在的区域——我感觉这个是非常重要的，算是从semantic segmentation的一个提升吧。这就和instance segmentation联系了起来，如何将每个人的区域都分割出来，相对semantic segmentation是要难不少的问题。基于semantic segmentation来做instance segmentation，大致做法是在dense feature map上面整合instance region proposal/score map/RoI，然后再进行分割。<br />
　　这里的instance segmentation本身是和object detection紧密相关的。Piotr Dollar(我也不知道是谁)有这样的一个观点：semantic segmentation is a bad direction,we should focus on object detection。<br />
　　总的来说：instance segmentation是semantic segmentation和object detection殊途同归的一个结合点，是一个非常重要的研究问题。<br />
　　如果说Instance Segmentation比Semantic Segmentation难，主要的原因应该是在网络结构的设计上。对于Semantic segmentation，现有结构基本上都是FCN及其变种的End2End的训练，是一个非常干净整洁的框架。原理也非常简单，就是一个per-pixel的分类问题。但是instance segmentation就会涉及到需要先做一遍detection然后再在Detection的结果里面估计segmentation的结果。如果单纯是detection的话，框架也可以是非常简单的。</p>

<h2 id="section-1">存在的一个问题</h2>
<ul>
  <li>在近10年来，以人工经验特征为主导的物体检测任务mAP(物体类别和位置的平均精度)提升非常缓慢。</li>
  <li>随着ReLu激活函数、dropout正则化手段和大规模图像样本的出现，CNN特征获得了较高的识别精确度</li>
  <li>在上述的前提下，引发了是否可以使用CNN来进行特征提取来提高当前一直停滞不前的物体检测准确率的思考。</li>
</ul>

<h2 id="section-2">介绍</h2>
<p>　　 特征是非常重要的，在过去的一段时间里面，各种视觉识别任务在过去的十年内取得了非常大的进步，这在很大程度上取决于SIFT和HOG的使用。但是在2010-2012年的这段时间里面，PASCAL VOC对象检测在这段时间里面的进展非常缓慢，仅仅通过组合不同模型和使用已有方法的变体来获得很小的改进。<br />
　　ImageNet挑战赛上，CNN的超强表现力使得CNN重新进入人们的视线。ImageNet的结果已发了一个问题：在多大程度上CNN分类对ImageNet的分类结果可以泛化为PASCAL VOC挑战的目标检测结果。<br />
　　和图像分类不同的是，目标检测需要在一张图片内对一个或是多个物体进行检测定位。一种方法是将检测任务归为回归问题，这种方法对对位单个物体能够有较好的表现力，但是在一张图片中检测多个物体需要更加复杂的框架。</p>

<h2 id="r-cnn">使用一个R-CNN来进行目标的检测</h2>
<p>　　paper中提出来的物体识别系统包含了三个模块。The first module generates category-independent region proposals(生成类别无关区域提案)These proposals define the set of candidate detections available to our detector.The second module is a convolutional network that extracts a fixed-length feature vector from each region.The third module is a set of class-specific linear SVMs.<br />
- region proposal:区域提案
- 第二个模块是从每个区域提取固定长度特征向量的大型卷积神经网络。
- 第三个模块是一组特定类别的线性SVM线性分类器。</p>

<h3 id="section-3">执行的步骤</h3>
<ul>
  <li>Input Image: 输入图像，采用Selective Search从原始图片中提取2000个左右区域候选框</li>
  <li>划分区域提案，进行归一化：将所有候选框变为固定大小的227*227区域 warp操作</li>
  <li>CNN网络提取特征。</li>
  <li>SVM结合NMS(非极大值抑制)识别分类区域边框，采用DPM精修边框的位置
<img src="/downloads/R-CNN.png" alt="" height="365" width="1024" />
　　Object detection system overview:The system takes an input image,extracts around 2000 bottom-up region proposals,computes features for each proposals using a large convolutional network, and then classifies each region using class-specific linear SVMs.</li>
</ul>

<h3 id="module-design">Module design</h3>
<ul>
  <li>Region proposals:论文中指出，现在存在很多的region proposal的方法，Examples include:objectness,selective search,category-independent object proposals,etc.While R-CNN is agnostic to the particular region proposal method,we use selective search to enable a controlled comparision with prior detection work.具体的区域提案方法对于R-CNN是透明的，但我们使用选择性搜索以便于与先前检测工作的对照比较。</li>
  <li>Feature extraction:In order to compute features for a region proposal,we must first convert the image data in that region into a form that is compatible with the CNNs.Of the many possible transformations of our arbitrary-shaped regions,we opt for the simplest.Regardless of the size or aspect ratio of the candidate region,we warp all pixels in a tight bounding box around it to the required size.<br />
　　第一步的region proposal提取完成了之后，我们就需要对region进行feature extraction。这里存在的一个问题就是我们必须首先将region中的data转换为适合进行CNN处理的格式。Warp处理进行的就是这样一个convert处理。<br />
　　Regardless of the size or aspect ratio(纵横比) of the candidate region, we warp all pixels in a tight bounding box around it to the required size.Prior to warping, we dilate(扩充，放大) the tight bounding box so that at the warped size there are exactly p pixels of warped image context around the original box(we use p=16).在进行变形之前，我们先在候选框周围加上16的padding，再进行各向异性缩放。<br />
　　Object proposal转换(warp):将各个Region的的大小变换到CNN的输入尺寸。</li>
</ul>

<h3 id="supervised-pre-training">Supervised pre-training</h3>
<p>　　To adapt out CNN to the new task and the new domain(warped proposal windows),we continue stochastic gradient descent(SGD) training of the CNN parameters using only warped region proposals.Aside from replacing the CNN’s ImageNet-specific 1000-way classification layer with a randomly initialized (N+1)-way classification layer(where N is the number of object classes while 1 for the background),the CNN architecture is unchanged.</p>

<h3 id="object-category-classifiers">Object category classifiers</h3>
<p>　　It is not clear how to label a region that parially overlap an object.The author resolved this issue with an IoU(intersection-over-union) threshold, below which regions are defined as negatives.The overlap threshold 0.3 was selected by a grid search over {0,0.1…0.5} on a validation set. We treat all region proposals with ≥ 0.5 IoU overlap with a ground-truth box positives for that box’s class and the rest as negatives.In each SGD iteration, we uniformly sample 32 positive windows(over all classes) and 96 background windows to construct a mini-batch of size 128.</p>

<h3 id="iou">IoU的解释</h3>
<p>　　在目标检测的评价体系中，有一个参数叫做IoU，简单来说就是模型产生的目标窗口和原来标记窗口的交叠率。具体我们可以理解为：检测结果和Ground Truth的交集并上它们的并集，就是检测的准确率IoU
<img src="/downloads/IoU.png" alt="" height="93" width="503" /></p>

<h3 id="section-4">博客链接</h3>
<p><a href="https://blog.csdn.net/hduxiejun/article/details/71107283">https://blog.csdn.net/hduxiejun/article/details/71107283</a><br />
<a href="https://blog.csdn.net/v1_vivian/article/details/80292569">bounding box</a></p>

<h3 id="iou-1">关于IoU的阈值设置问题</h3>
<p>　　在看论文的时候就发现了，这个IoU的阈值设置出现了好多不同的值，比较典型的就是0.3和0.5这两个值，刚开始的时候也没有搞清楚这两个值之间使用的区别，看github的时候找到了一个小的总结。
- finetune过程：计算每个region proposal和人工标注的框的IoU，IoU重叠阈值设置为0.5，大于这个阈值的设置为正样本，其余的设置为负样本。然后在训练的时候，每一次迭代中都使用32个正样本(包括所有类别)和96个背景样本组成的128张图片的batch进行训练。
- SVM的训练：对每个类都训练一个SVM分类器，训练SVM需要正负样本，这里的正样本就是ground-truth框中的图像作为正样本，完全不包含的region proposal应该是负样本，但是对于部分包含某一类物体的region proposal应该如何训练——同样是使用IoU阈值的方法，这时的阈值设置为0.3，计算每一个region proposal和标准框的IoU，小于0.3的作为负样本，其余的全部丢弃。
- 关于为什么两个阈值是不一样的问题：作者提到刚开始的时候是使用了ImageNet预训练了CNN，用提取的特征训练了SVMs，此时用正负样本标记方法就是上面说的0.3，后面在进行fine-tune的时候，使用了这个方法，但是发现这样做的效果并不理想，于是通过调试选择了0.5这个阈值。
- hard negatives: 在训练过程中会出现 正样本的数量远远小于负样本，这样训练出来的分类器的效果总是有限的，会出现许多false positive。采取办法可以是，先将正样本与一部分的负样本投入模型进行训练，然后将训练出来的模型去预测剩下未加入训练过程的负样本，当负样本被预测为正样本时，则它就为false positive，就把它加入训练的负样本集，进行下一次训练，直到模型的预测精度不再提升。这就好比错题集，做错了一道题，把它加入错题集进行学习，学会了这道题，成绩就能得到稍微提升，把自己的错题集都学过去，成绩就达到了相对最优。</p>

<h2 id="bounding-box-regression">Bounding Box Regression</h2>
<p>　　我们使用一个简单的bounding box回归来提高定位的准确性。先给出两个博客的链接<a href="https://blog.csdn.net/zijin0802034/article/details/77685438">https://blog.csdn.net/zijin0802034/article/details/77685438</a><br /><a href="https://blog.csdn.net/Bixiwen_liu/article/details/53840913">https://blog.csdn.net/Bixiwen_liu/article/details/53840913</a><br />
　　As the paper points out: After scoring each selective search proposal with a class-specific detection SVM, we predict a new bounding box for the detection using a class-specific bounding box regressor.<br />
　　回归的这一步的输入的是一组参数，分别表示窗口的中心点坐标和宽高和ground truth对应窗口的中心点坐标和宽高。我们需要做的是寻找一种映射关系使得输入的原始窗口P经过映射之后得到一个更加真实窗口G更加接近的回归窗口。我们的边框回归学习的主要就是四个变换。首先是平移变换，然后是尺度缩放。<br />
　　看完了相关的博客还有paper中的内容，基本上理解了BBX的算法的实现思路。整理一下写下来：首先基本的就是上面两个大思路，先计算得到平移，然后再计算得到尺度缩放，我们如何得到对应的平移和尺度缩放分别对应的四个变换呢？需要我们设计算法来实现这四个映射。采用的思路是基于回归进行的。<br />
　　那么回归中的四个目标值对应的是什么呢？对应的是真正需要的平移量和尺度缩放量，对应的是paper中的6~9。那么对应的预测值是什么呢？按照paper中的讲解来说是：Each function is modeled as a linear funciton of the pool5 features of proposal P.我们的输入实际上是proposal对应的CNN的特征，对应一组可学习的参数，最小二乘法进行参数的调优。最终找到四个变换的linear function.<br />
　　We learn W* by optimizing the regularized least squares objective:
<img src="/downloads/BBX regression.png" alt="" height="70" width="400" /></p>

<h3 id="som">Som</h3>
<ul>
  <li>关于宽高为什么设计为上面的那种形式：首先是因为CNN具有尺度不变性(scale invariant)。意思是对应同一个图像，大小不一样。经过CNN之后提取出来的特征应该是相同的，我们将相同的变量输入到函数中，得到的函数的结果也一定是相同的。但是对于两个不同尺寸的图片来说，尺度变换以及平移对应的结果一定是不相同的，所以在除以了宽和高。</li>
  <li>存在的另一个问题，也是我在没有看到解释之前一直没有想明白的一个问题：我们如何选择合适的region proposal以及Ground truth对。作者给出的答复是：We only learn from a proposal P if it is nearby at least one ground-truth box.We implement “nearness” by assigning P to the ground-truth box G with which it has maximum IoU overlap if and only if the overlap is greater than a threshold.也就是我们会选择超过IoU阈值的最大重叠率的region proposal进行regression操作。We do this once for each object class in order to learn at a set of class-specific bounding-box regressors.</li>
</ul>

<h2 id="section-5">分割线2018-09-10</h2>
<p>　　今天重新看论文的时候，发现了一个对于自己理解的时候很重要的一个点：
<img src="/downloads/RCNN fatch.png" alt="" height="200" width="410" />
　　给出来一幅图中的regions的scroes，如果和它的IoU的值超过阈值的一个region对应的score更高，前者的region会被舍弃。所以我们最后看到的region并不会非常多。
　　重新看了代码并反复读了论文之后，对整个框架的脉络算是梳理的有些思路了。整理下来：
- 首先我们需要训练一个卷积神经网络，这时使用的数据集是分类任务的数据集，在github上面的代码中，我么可以看到，使用的是17flowers的数据集，对应的labels是花的种类，这个时候并没有使用bounding box的数据集，印证了论文中的内容。
- 接下来是fine-tune的一个过程，这个时候使用到的数据集对应的是region proposals及其对应的labels。这个时候使用的IoU的threshold的值是0.5
- 然后是训练SVM分类器，同样输入的是region proposals及其对应的labels。这个时候threshold的值设置的是0.3。
- 最后我们对bounding box进行调整，使用的是回归的方法。</p>

    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/SVS%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86">SVS算法整理</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0(%E4%B8%89)">DeepLearning复习(三)</a></li>
        
            <li><a href="/Jetson_TX2%E5%BC%80%E5%8F%91">Jetson_TX2开发</a></li>
        
            <li><a href="/DarkNet%E5%AD%A6%E4%B9%A0">DarkNet深度框架学习</a></li>
        
            <li><a href="/C++(%E4%B8%80)">C++复习（一）</a></li>
        
            <li><a href="/%E5%9C%A8linux%E4%B8%8A%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAC++%E7%9A%84%E7%A8%8B%E5%BA%8F">在linux上使用Vim编写一个C/C++程序</a></li>
        
            <li><a href="/Ubuntu16.04%E4%B8%8B%E5%AE%89%E8%A3%85cuda-cudnn">ubuntu16.04安装显卡驱动及cuda</a></li>
        
            <li><a href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1">研究生的数学建模</a></li>
        
            <li><a href="/Faster_RCNN">Faster RCNN</a></li>
        
            <li><a href="/Fast-RCNN">Fast R-CNN</a></li>
        
            <li><a href="/R-CNN">R-CNN学习笔记</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0%E4%BA%94">DeepLearning复习(五)</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0%E5%9B%9B">DeepLearning(四)</a></li>
        
            <li><a href="/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%A4%8D%E4%B9%A0">Deeplearning复习(三)</a></li>
        
            <li><a href="/%E4%B8%80%E6%AE%B5%E5%A5%94%E6%B3%A2%E4%B9%8B%E5%90%8E">一段奔波结束</a></li>
        
            <li><a href="/%E9%99%88%E8%80%81%E5%B8%88%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93">陈老师的论文简述</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0-%E4%BA%8C">DeepLearning复习(二)</a></li>
        
            <li><a href="/%E5%8F%88%E6%98%AF%E4%B8%80%E7%AF%87%E5%85%B3%E4%BA%8E%E4%BF%9D%E7%A0%94%E7%9A%84%E9%9A%8F%E6%84%9F">又是保送过程中的一点随感</a></li>
        
            <li><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">大数据和机器学习学习笔记</a></li>
        
            <li><a href="/%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%AB%AF%E5%8D%88%E5%87%BA%E8%A1%8C">一个人的端午出游</a></li>
        
            <li><a href="/%E8%BF%98%E6%98%AF%E5%8C%97%E4%BA%AC">还是北京</a></li>
        
            <li><a href="/FaceNet%E5%AD%A6%E4%B9%A0">FaceNet学习</a></li>
        
            <li><a href="/%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AF%94%E8%BE%83">分类和回归之间的比较</a></li>
        
            <li><a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0">神经网络的复习</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0-%E4%B8%80">DeepLearning复习（一）</a></li>
        
            <li><a href="/BinarySearchTree">Binary Search Tree</a></li>
        
            <li><a href="/%E5%B0%86%E6%9D%A5%E7%9A%84%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%84%9F%E8%B0%A2%E7%8E%B0%E5%9C%A8%E5%8A%AA%E5%8A%9B%E7%9A%84%E8%87%AA%E5%B7%B1">将来的你一定会感谢现在努力的自己</a></li>
        
            <li><a href="/PSPNet%E7%9A%84%E5%AD%A6%E4%B9%A0">PSP网络的学习</a></li>
        
            <li><a href="/ResNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0">ResNet网络的学习</a></li>
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/tensorflow%E7%9A%84session">tensorflow的session</a></li>
        
            <li><a href="/protobuf">protobuf</a></li>
        
            <li><a href="/stringstream">stringstream</a></li>
        
            <li><a href="/C++%E4%B8%AD%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%AC%A6">C++中的几个操作符</a></li>
        
            <li><a href="/pip3_error">pip3更新之后出错</a></li>
        
            <li><a href="/%E7%94%A8AD%E7%94%BBPCB%E6%9D%BF">使用AD制作PCB板</a></li>
        
            <li><a href="/cuda%E5%92%8Ccudnn">什么是cuda和cuDNN</a></li>
        
            <li><a href="/%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85ubuntu16.04.05">ubuntu16.04.5的安装</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E7%95%8C%E8%AF%BB%E5%8F%96%E5%8F%82%E6%95%B0">python解析命令行参数</a></li>
        
            <li><a href="/anaconda%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85">anaconda安装第三方包</a></li>
        
            <li><a href="/%E5%8D%B7%E7%A7%AF%E5%92%8C%E6%BB%A4%E6%B3%A2%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB">卷积和滤波之间的区别</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98">参数初始化的问题</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98">参数和超参数的问题</a></li>
        
            <li><a href="/%E6%9D%82%E8%AE%B0%E9%9A%8F%E7%AC%94">杂记随笔</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E8%B4%AB%E5%AF%8C%E5%B7%AE%E8%B7%9D">关于贫富差距的感想</a></li>
        
            <li><a href="/%E4%BE%9D%E7%84%B6%E5%9B%9E%E5%BD%92%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E5%B7%AE%E5%88%AB%E7%9A%84%E9%97%AE%E9%A2%98">依然是SVM和线性回归之间的对比问题</a></li>
        
            <li><a href="/%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8">训练集验证集和测试集的作用</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%89">C语言复习三</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%AF%BC%E5%85%A5%E5%A4%B4%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98">关于导入头文件的问题</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%BA%8C">C语言复习二</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%80">C语言复习一</a></li>
        
            <li><a href="/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E8%BD%BD">加载预训练好的模型时遇到的问题</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E9%83%A8%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0">Python从外部传入参数</a></li>
        
            <li><a href="/%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B">如何保存一个训练好的模型或是参数</a></li>
        
            <li><a href="/CV%E6%8C%91%E6%88%98%E8%B5%9B">CV中非常重要的挑战赛</a></li>
        
            <li><a href="/%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95">图论算法</a></li>
        
            <li><a href="/%E8%BF%91%E4%B8%A4%E5%A4%A9%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%84%9F%E6%83%B3">近两天对深度学习的一点感想</a></li>
        
            <li><a href="/benchmark">几个概念的理解</a></li>
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
            <li><a href="/%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93">通信原理复习总结</a></li>
        
            <li><a href="/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1">课程设计</a></li>
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
