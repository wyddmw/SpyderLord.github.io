<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>ROI_depth_estimation | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/ROI_depth_fusion" title="ROI_depth_estimation">ROI_depth_estimation</a></h1>
        <p class="entry-date">2019-03-05</p>
        <h2 id="multi-task-training-for-object-detection-and-depth-estimation-aimed-at-roi-depth-estimation-based-on-a-monocular-image">Multi-task training for object detection and depth estimation aimed at ROI depth estimation based on a monocular image</h2>
<p>　　先来一个大气又冗长的标题，基本上表达了想要设计网络模型的目的是什么——设计一个多任务联合训练的模型，模型的输出是两个分支，分别对应的是目标检测任务和深度估计任务。但是这并不是网络的全部内容，只是在训练阶段的模型结构，是一个多任务encoder-decoder的结构，在预测的时候，并不会在全局上进行深度的估计，建立在目标检测得到的ROI的基础上，只在特定的区域内进行深度的估计，减少模型的运算量，下面会给出模型在训练和预测时候各自对应的网络结构。<br /></p>

<h2 id="multi-task-training-network-in-tensorflow">Multi-task training network in tensorflow</h2>
<p>　　多任务联合训练通过tensorflow的编程是可以实现的：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># numpy制造两组假数据
</span><span class="n">x_data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">100</span><span class="p">))</span>
<span class="n">y1_data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mf">0.100</span><span class="p">,</span><span class="mf">0.200</span><span class="p">],</span><span class="n">x_data</span><span class="p">)</span><span class="o">+</span><span class="mf">0.300</span>
<span class="n">y2_data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">([</span><span class="mf">0.500</span><span class="p">,</span><span class="mf">0.900</span><span class="p">],</span><span class="n">x_data</span><span class="p">)</span><span class="o">+</span><span class="mf">3.00</span>

<span class="n">b1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">w1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))</span>
<span class="n">y1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span><span class="n">x_data</span><span class="p">)</span><span class="o">+</span><span class="n">b1</span>

<span class="n">b2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">w2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_unifor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">))</span>
<span class="n">y2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">x_data</span><span class="p">)</span><span class="o">+</span><span class="n">b2</span>

<span class="c1">#损失函数部分
</span><span class="n">loss_1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">((</span><span class="n">y1</span><span class="o">-</span><span class="n">y1_data</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">loss_2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">((</span><span class="n">y2</span><span class="o">-</span><span class="n">y2_data</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1">#构造优化器
</span><span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">train1</span><span class="o">=</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_1</span><span class="p">)</span>
<span class="n">train2</span><span class="o">=</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss_2</span><span class="p">)</span>

<span class="c1">#初始化全局变量
</span><span class="n">init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="c1">#如果我们只是想执行其中的一组数据，尝试一下：
</span>        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">w1</span><span class="p">),</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">b1</span><span class="p">))</span>
                   
<span class="c1">#多任务联合训练的方法有两种，一种是交替训练，另一种是何在一起训练，根据我们的数据集是什么样的，选择不同的训练方式
</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1">#定义占位符
</span><span class="n">x</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">'float'</span><span class="p">,[</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span><span class="n">name</span><span class="o">=</span><span class="s">'X'</span><span class="p">)</span>
<span class="n">y1</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">'float'</span><span class="p">,[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">name</span><span class="o">=</span><span class="s">'Y1'</span><span class="p">)</span>
<span class="n">y2</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="s">'float'</span><span class="p">,[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">name</span><span class="o">=</span><span class="s">'Y2'</span><span class="p">)</span>

<span class="c1">#定义权重
</span><span class="n">initial_shared_layer_weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>	<span class="c1"># the shape is 10*20
</span><span class="n">initial_Y1_layer_weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">initial_Y2_layer_weight</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>

<span class="n">shared_layer_weights</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_shared_layer_weight</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'share_W'</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">Y1_layer_weights</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_Y1_layer_weight</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Layer1'</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">Y2_layer_weights</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_Y2_layer_weight</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="s">'Layer2'</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>

<span class="c1">#使用ReLU来构建layer
</span><span class="n">shared_layer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">shared_layer_weights</span><span class="p">))</span>
<span class="n">y1_layer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">shared_layer</span><span class="p">,</span><span class="n">Y1_layer_weights</span><span class="p">))</span>
<span class="n">y2_layer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">shared_layer</span><span class="p">,</span><span class="n">Y2_layer_weights</span><span class="p">))</span>

<span class="c1">#计算损失函数
</span><span class="n">y1_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">y1</span><span class="o">-</span><span class="n">y1_layer</span><span class="p">)</span>
<span class="n">y2_loss</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">y2</span><span class="o">-</span><span class="n">y2_layer</span><span class="p">)</span>

<span class="c1">#优化器
#Y1_op=tf.train.AdamOptimizer().minimize(y1_loss)
#Y2_op=tf.train.AdamOptimizer().minimize(y2_loss)
</span><span class="n">Joint_loss</span><span class="o">=</span><span class="n">y1_loss</span><span class="o">+</span><span class="n">y2_loss</span>
<span class="n">optimiser</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">Joint_loss</span><span class="p">)</span>
<span class="n">Y1_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">y1_loss</span><span class="p">)</span>
<span class="n">Y2_op</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">y2_loss</span><span class="p">)</span>


<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">()</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="c1">#专门做了这样的测试，也即是在feed_dict的时候可以只输入其中一条分支所需要的数据
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="n">_</span><span class="p">,</span><span class="n">Y1_loss</span><span class="o">=</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">Y1_op</span><span class="p">,</span><span class="n">y1_loss</span><span class="p">],</span><span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
            <span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span>
            <span class="n">y1</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
        <span class="p">})</span>
        <span class="k">print</span><span class="p">(</span><span class="n">Y1_loss</span><span class="p">)</span>


</code></pre></div></div>

<h2 id="i-have-something-to-say">I have something to say</h2>

<p>　　昨晚上在朋友圈看到有同学被康奈尔大学研究生录取了，当然了首先应该为这位同学被世界顶级学府录取感到高兴，高兴之余又想了点东西。我和他的方向还有些像，都是和深度学习相关的，具体他是做视觉还是自然语言处理我记不清了，我保送的时候有见过这位同学，我当时还没有任何着落，在国内的一些院校中疯狂地发着简历和邮件。问他想去哪，当时有提到帝国理工，好像也有康奈尔。当时他也是在做着准备工作，等待这offer。对于优秀的标准，我总是不经意习惯将吴丹作为一个评价的标准，因为都是计算机专业，她作为一个女生有着自己成熟的心智还有坚强，她也要出国了，但是最近没有什么联系，不知道现在进展如何了。当时她告诉我的是，感觉top级别的大学申请不下来了，因为没有科研经历不够，也没有什么论文。今天早上和黄子纯的morning call，我们聊到了这个问题。对于科研这个概念，我是很模糊的。我觉得说道科研，首先要有自己的创新点和想法在里面。如果只是照着课本重复性的去做一些实验的复现，或是将现成的东西应用到某一个具体的问题，我觉得那叫做工程或者动手更合适一些。站在距离毕业还有四个月左右的节点上，我自己的大学生活中又做了哪些东西呢？好像一直在动手，但是依然手残，C++不会写，tensorflow读不懂，曾经引以为傲的一个项目——大学生科创计划，后来和一起完成的朋友说起来时，也一致觉得，根本没有什么好值得骄傲的。但好在自己能吹的天花乱坠，蒙骗了一些老师，让我最后叩开了同济的大门。在没有开始实习之前，可以说自己是没有什么创造力的吧。在自动化所里，一遍做着目标检测的学习，一边完成着同济老师布置的深度估计的学习——我常给黄子纯说自己是同济和中科院联合培养的学生，她戏谑说因为太弱了所以需要两个高校来一起培养(手动捂脸.jpg)。当时身边的同事做的是毫米波雷达的融合，和距离的检测也有关系，就问他障碍物检测这个任务对于深度距离的信息是不是也很需要，其实自己想想也是非常重要的。只是在二维的平面做目标检测其实实用性是很低的，没有深度信息，映射不到世界坐标系中，对于车辆上层的决策其实还是缺少有效的信息。当时就想了，如果我能做出来深度估计和目标检测端到端的输出，那岂不是省去了后期和激光雷达融合的步骤了吗？当时的出发点是为了减少运算量，提高模型的实时性，更简单地说，更像是去做拼图，将两个模块拼起来。虽然之前看过一个多任务联合训练的论文MultiNet，但是对Multi-task这样的任务并没有什么概念，只是知道可以一个输入，多个输出，稀里糊涂就想做一个这样的模型。最开始的时候给贾臻伟说了自己的想法，觉得是一个不错的想法。给同济的老师说了之后更是得到了大力支持，同意我毕设的时候自己做这个题目。随着任务的不断推进，看了如何联合训练的方法，了解到多任务的模型在训练的时候是不同任务之间是可以相互影响到的，意识到如果加入到深度的信息说不定可以改进目标检测的效果。开心啊，说不定自己的这个想法就改变世界了？而且我最开始设计的模型是在检测出目标物体的区域内做深度的估计，原因也很简单，从人的行为出发，我们也不需要得到视线所及范围全部区域的深度信息，我觉得只是在影响到我们做决策的区域的深度信息才是有用的，所以在全局上做深度的估计是一种计算的浪费。</p>

<p>　　早上起来看一个公众号，讲的是3D-BBX在目标检测中的方法，在2018年的CVPR等会议中的论文中，有三篇论文都是将深度的信息加以融入。其中微软亚洲研究院的论文更是直接提出了instance depth的概念，做的事情就是我想的在ROI区域里面进行深度估计的事情。不禁为自己从产生这个想法到一步步写程序去验证想法到现在开始做自己的模型融合感到开心，在没有任何资料查询的基础上，想出来的这个idea没想到也是很有 价值的一个点，而且微软还把它做出来了，发了CVPR。</p>

<p>　　写了这么多，要接上最开始的内容——不要让自己所处的环境限制了自己的发展，在常春藤也好，在国内重本也好，人才是决定问题的关键因素，所以不要让自己的进步被环境拖了后腿，外界环境不背锅。</p>

    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/letting-go">letting go</a></li>
        
            <li><a href="/HappyNewYear">Happy New Year</a></li>
        
            <li><a href="/FLaME%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3">FlaME论文阅读理解</a></li>
        
            <li><a href="/%E4%B8%89%E7%BB%B4%E5%88%9A%E4%BD%93%E7%9A%84%E8%BF%90%E5%8A%A8">刚体运动，视觉SLAM的观测方程</a></li>
        
            <li><a href="/%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%9A%84%E7%A8%8B%E5%BA%8F">加载预训练好的模型进行预测</a></li>
        
            <li><a href="/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95">SSD中的一些重要的算法</a></li>
        
            <li><a href="/SSD">SSD-MobileNet</a></li>
        
            <li><a href="/SVS%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86">SVS算法整理</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0(%E4%B8%89)">DeepLearning复习(三)</a></li>
        
            <li><a href="/Jetson_TX2%E5%BC%80%E5%8F%91">Jetson_TX2开发</a></li>
        
            <li><a href="/C++(%E4%B8%80)">C++复习（一）</a></li>
        
            <li><a href="/%E5%9C%A8linux%E4%B8%8A%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAC++%E7%9A%84%E7%A8%8B%E5%BA%8F">在linux上使用Vim编写一个C/C++程序</a></li>
        
            <li><a href="/Ubuntu16.04%E4%B8%8B%E5%AE%89%E8%A3%85cuda-cudnn">ubuntu16.04安装显卡驱动及cuda</a></li>
        
            <li><a href="/%E7%A0%94%E7%A9%B6%E7%94%9F%E7%9A%84%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1">研究生的数学建模</a></li>
        
            <li><a href="/Faster_RCNN">Faster RCNN</a></li>
        
            <li><a href="/Fast-RCNN">Fast R-CNN</a></li>
        
            <li><a href="/R-CNN">R-CNN学习笔记</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0%E4%BA%94">DeepLearning复习(五)</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0%E5%9B%9B">DeepLearning(四)</a></li>
        
            <li><a href="/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%A4%8D%E4%B9%A0">Deeplearning复习(三)</a></li>
        
            <li><a href="/%E4%B8%80%E6%AE%B5%E5%A5%94%E6%B3%A2%E4%B9%8B%E5%90%8E">一段奔波结束</a></li>
        
            <li><a href="/%E9%99%88%E8%80%81%E5%B8%88%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93">陈老师的论文简述</a></li>
        
            <li><a href="/Deeplearning%E5%A4%8D%E4%B9%A0-%E4%BA%8C">DeepLearning复习(二)</a></li>
        
            <li><a href="/%E5%8F%88%E6%98%AF%E4%B8%80%E7%AF%87%E5%85%B3%E4%BA%8E%E4%BF%9D%E7%A0%94%E7%9A%84%E9%9A%8F%E6%84%9F">又是保送过程中的一点随感</a></li>
        
            <li><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">大数据和机器学习学习笔记</a></li>
        
            <li><a href="/%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E7%AB%AF%E5%8D%88%E5%87%BA%E8%A1%8C">一个人的端午出游</a></li>
        
            <li><a href="/%E8%BF%98%E6%98%AF%E5%8C%97%E4%BA%AC">还是北京</a></li>
        
            <li><a href="/FaceNet%E5%AD%A6%E4%B9%A0">FaceNet学习</a></li>
        
            <li><a href="/%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AF%94%E8%BE%83">分类和回归之间的比较</a></li>
        
            <li><a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0">神经网络的复习</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0-%E4%B8%80">DeepLearning复习（一）</a></li>
        
            <li><a href="/BinarySearchTree">Binary Search Tree</a></li>
        
            <li><a href="/%E5%B0%86%E6%9D%A5%E7%9A%84%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%84%9F%E8%B0%A2%E7%8E%B0%E5%9C%A8%E5%8A%AA%E5%8A%9B%E7%9A%84%E8%87%AA%E5%B7%B1">将来的你一定会感谢现在努力的自己</a></li>
        
            <li><a href="/PSPNet%E7%9A%84%E5%AD%A6%E4%B9%A0">PSP网络的学习</a></li>
        
            <li><a href="/ResNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0">ResNet网络的学习</a></li>
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%B8%93%E9%97%A8%E5%88%B6%E4%BD%9C%E5%9C%A8%E5%B8%B8%E7%86%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86">为什么要专门制作场数据集</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E7%82%B9%E4%BA%91SLAM">激光雷达SLAM</a></li>
        
            <li><a href="/%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E9%87%8D%E5%91%BD%E5%90%8D">对文件进行重命名</a></li>
        
            <li><a href="/opencv%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E8%84%9A%E6%9C%AC">opencv的一个小小脚本</a></li>
        
            <li><a href="/%E4%B8%80%E4%B8%AAidea">目标检测和深度估计的结合</a></li>
        
            <li><a href="/object_detection_api">使用tensorflow的API直接进行训练</a></li>
        
            <li><a href="/tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96">TensorBoard可视化查看模型的graph</a></li>
        
            <li><a href="/scope">scope和variable</a></li>
        
            <li><a href="/IoU&&NMS">Code of IoU and NMS</a></li>
        
            <li><a href="/tensorflow%E7%9A%84session">tensorflow的session</a></li>
        
            <li><a href="/protobuf">protobuf</a></li>
        
            <li><a href="/stringstream">stringstream</a></li>
        
            <li><a href="/C++%E4%B8%AD%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%AC%A6">C++中的几个操作符</a></li>
        
            <li><a href="/pip3_error">pip3更新之后出错</a></li>
        
            <li><a href="/%E7%94%A8AD%E7%94%BBPCB%E6%9D%BF">使用AD制作PCB板</a></li>
        
            <li><a href="/cuda%E5%92%8Ccudnn">什么是cuda和cuDNN</a></li>
        
            <li><a href="/%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85ubuntu16.04.05">ubuntu16.04.5的安装</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E7%95%8C%E8%AF%BB%E5%8F%96%E5%8F%82%E6%95%B0">python解析命令行参数</a></li>
        
            <li><a href="/anaconda%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85">anaconda安装第三方包</a></li>
        
            <li><a href="/%E5%8D%B7%E7%A7%AF%E5%92%8C%E6%BB%A4%E6%B3%A2%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB">卷积和滤波之间的区别</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98">参数初始化的问题</a></li>
        
            <li><a href="/%E5%8F%82%E6%95%B0%E5%92%8C%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98">参数和超参数的问题</a></li>
        
            <li><a href="/%E6%9D%82%E8%AE%B0%E9%9A%8F%E7%AC%94">杂记随笔</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E8%B4%AB%E5%AF%8C%E5%B7%AE%E8%B7%9D">关于贫富差距的感想</a></li>
        
            <li><a href="/%E4%BE%9D%E7%84%B6%E5%9B%9E%E5%BD%92%E5%92%8C%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E5%B7%AE%E5%88%AB%E7%9A%84%E9%97%AE%E9%A2%98">依然是SVM和线性回归之间的对比问题</a></li>
        
            <li><a href="/%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E4%BD%9C%E7%94%A8">训练集验证集和测试集的作用</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%89">C语言复习三</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%AF%BC%E5%85%A5%E5%A4%B4%E6%96%87%E4%BB%B6%E7%9A%84%E9%97%AE%E9%A2%98">关于导入头文件的问题</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%BA%8C">C语言复习二</a></li>
        
            <li><a href="/C%E8%AF%AD%E8%A8%80%E5%A4%8D%E4%B9%A0%E4%B8%80">C语言复习一</a></li>
        
            <li><a href="/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E8%BD%BD">加载预训练好的模型时遇到的问题</a></li>
        
            <li><a href="/python%E4%BB%8E%E5%A4%96%E9%83%A8%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0">Python从外部传入参数</a></li>
        
            <li><a href="/%E5%A6%82%E4%BD%95%E4%BF%9D%E5%AD%98%E4%B8%80%E4%B8%AA%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B">如何保存一个训练好的模型或是参数</a></li>
        
            <li><a href="/CV%E6%8C%91%E6%88%98%E8%B5%9B">CV中非常重要的挑战赛</a></li>
        
            <li><a href="/%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95">图论算法</a></li>
        
            <li><a href="/%E8%BF%91%E4%B8%A4%E5%A4%A9%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%84%9F%E6%83%B3">近两天对深度学习的一点感想</a></li>
        
            <li><a href="/benchmark">几个概念的理解</a></li>
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
            <li><a href="/ROI_depth_fusion">ROI_depth_estimation</a></li>
        
            <li><a href="/%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93">通信原理复习总结</a></li>
        
            <li><a href="/%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1">课程设计</a></li>
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
