<!DOCTYPE html>
<html>
<head>
    <!--
    * Author:         BeiYuu
    * Revised:        Mukosame
    -->
    <meta charset="utf-8" />
    <title>Overfitting | Spyder's blog</title>
    <meta name="author" content="SpyderLord" />
    <meta name="renderer" content="webkit">
    <meta name="description" content="Everything about SpyderLord" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <link rel="stylesheet" href="/css/default.css" type="text/css" />
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/.vscode" />
    <script src="/js/jquery-1.7.1.min.js" type="text/javascript"></script>
</head>
<body>

    <div class="home-menu">
        <div class="home-icon-con">
            <a class="home-menu-icon" href="/">SpyderLord</a>
            <a class="home-follow" href="#" title="Contact Me">+</a>
        </div>
        <div class="home-contact">
            <a href="https://github.com/SpyderLord/" target="_blank" style="margin-left:-5px;"><img src="https://github.com/favicon.ico" alt="" width="22"/></a>
           <!-- <a href="http://www.zhihu.com/people/xiang-xiao-yu-20" target="_blank" style="text-align:right"><img src="http://www.zhihu.com/favicon.ico" alt="" width="22"/></a>-->
        </div>
    </div>

    <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
    body { background:#e8e8e8; }
    @media screen and (max-width: 750px){
        body { background:#fff; }
    }
    @media screen and (max-width: 1020px){
        body { background:#fff; }
    }
</style>

<div id="content">
    <div class="entry">
        <h1 class="entry-title"><a href="/overfit" title="Overfitting">Overfitting</a></h1>
        <p class="entry-date">2018-03-29</p>
        <h2 id="什么是过拟合">什么是过拟合？</h2>
<ul>
  <li>个人看来，过拟合就是训练的模型在训练集上跑的效果非常理想，但是在测试集上的效果却差强人意，很明显，模型只是记住了训练集中的数字，但是对于数字背后体现出来的特征并没有真正理解，一个直接的原因就是训练的次数太多了，所以这也就体现出来了一个问题训练的次数并不是越多越好，斯坦福的教材中提到过，需要将模型的复杂程度和训练的次数结合起来，二者之间是存在紧密联系的。</li>
  <li>存在着紧密联系？仔细想一下好像确实是这样的，如果我们在某一层卷积层中使用的卷积核的数量很多，就意味着我们想要提取多个特征，所以我们多训练几次也是解释的通的，就好比我们人在识别一张包含的非常大信息量的图片的时候，我们也会多看几次去做好识别，但是对于一张非常简单的图片来说如果我们每次都非常仔细的去看，而且还看非常多次数，那么我们将图片上一些非常不起眼的小细节会去注意到也就不足为奇了。<br />
    <h2 id="在实际的程序代码我们从什么地方看出来会产生过拟合">在实际的程序代码我们从什么地方看出来会产生过拟合。</h2>
  </li>
  <li>在MNIST手写是别题的训练中，我们每次使用的minibatch的尺寸是50,然后我们迭代的次数是20000次，加起来这就是1000000次，我记得分给训练用的数据量是50000,按照这样计算，平均每张图都会学习20次。我之前想的问题是，因为每次都会选取不同的minibatch，就相当于是每次都在给模型一个测试集。为什么会在训练的时候正确率很高但是在测试的时候正确率很低呢？问题就在这样的一个数据上，其实在训练的次数没有到一个batch数据量的时候，对于模型来说，每一个batch都相当于是一个testbatch，如果我们这个时候去输出训练集和测试集上的数据进行一个比对的话，其实测试集和训练集的正确率是非常接近的，而且按照梯度下降进行优化的原则来看，测试集上的正确率比训练集上的正确率还高也是有可能的。</li>
  <li>我们也在电脑上按照自己的想法进行了验证，之前的迭代次数是20000,如果我们改成了151,然后minibatch的选择是30,然后将每100次的结果输出一次，在训练集上的正确率不断地进行上升，然后在输出测试集上正确率的时候数值确实也非常的相近。<br />
    <h2 id="如何来看什么时候出现了过拟合呢">如何来看什么时候出现了过拟合呢？</h2>
  </li>
  <li>这个问题在谷歌上搜索了一下之后看到了这样的回答，我们将在训练集上的正确率和在测试集上的正确率都都打印出来，如果在训练集上的正确率不断地升高，但是在测试集上的正确率却出现走低，就意味着可能是过拟合了。</li>
  <li>写到这里，忽然又想到了一个问题，因为我们将一个完整的数据集分成了test、train还有一个validation，那么validation这个数据集说是起到的是验证的作用，那会不会就是通过这个validation来帮助我们观察是否出现过拟合呢？还会进一步去想这个问题的。</li>
  <li><a href="http://blog.csdn.net/u010167269/article/details/51340070">http://blog.csdn.net/u010167269/article/details/51340070</a>这个博客中写的内容还是比较详细的，validation确实是具备防止过拟合作用的，上面自己的想法还是正确的～但是除了这个功能之外，validation还有其他的作用！</li>
</ul>

<h2 id="分割线-2018-04-19">分割线 2018-04-19</h2>
<ul>
  <li>这两天在训练FCN网络的时候发现了一个问题，我们应该如何看loss的值。想起来之前在notes中看到的关于训练方法的问题，我们应该先用小数量的样本进行训练，然后训练到过拟合为止。想到这里就想起来一个问题，我们为什么要防止过拟合。从loss的结果来看，就是train的数据集上的值不断升高，但是在测试集上的损失值却从出现走低的现象可能就是出现了过拟合。之所以要避免过拟合现象的出现就是因为我们在训练数据集上的测试效果虽然很好，但是范化能力很差的现象。这样的模型我们是不需要的。我们要训练出来的是范化能力强的网络。</li>
</ul>

    </div>

    <div class="sidenav">
        <h2>Blog</h2>
        <ul class="artical-list">
        
            <li><a href="/FaceNet%E5%AD%A6%E4%B9%A0">FaceNet学习</a></li>
        
            <li><a href="/%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AF%94%E8%BE%83">分类和回归之间的比较</a></li>
        
            <li><a href="/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0">神经网络的复习</a></li>
        
            <li><a href="/DeepLearning%E5%A4%8D%E4%B9%A0">DeepLearning复习</a></li>
        
            <li><a href="/BinarySearchTree">Binary Search Tree</a></li>
        
            <li><a href="/%E5%B0%86%E6%9D%A5%E7%9A%84%E4%BD%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%84%9F%E8%B0%A2%E7%8E%B0%E5%9C%A8%E5%8A%AA%E5%8A%9B%E7%9A%84%E8%87%AA%E5%B7%B1">将来的你一定会感谢现在努力的自己</a></li>
        
            <li><a href="/PSPNet%E7%9A%84%E5%AD%A6%E4%B9%A0">PSP网络的学习</a></li>
        
            <li><a href="/ResNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0">ResNet网络的学习</a></li>
        
            <li><a href="/MultiNet%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A1%A5%E5%85%85">MultiNet网络学习的补充</a></li>
        
            <li><a href="/Optimization">Optimization</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8A%9B">关于执行力</a></li>
        
            <li><a href="/FCN_Net%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%80%E7%82%B9%E8%A1%A5%E5%85%85">FCN_Net的补充</a></li>
        
            <li><a href="/FCN">FCN_Net</a></li>
        
            <li><a href="/%E6%B8%85%E6%98%8E%E5%9C%A8%E5%8C%97%E4%BA%AC">清明在北京</a></li>
        
            <li><a href="/%E5%85%B3%E4%BA%8E%E5%86%99%E8%87%AA%E8%8D%90%E4%BF%A1">关于写保研自荐信</a></li>
        
            <li><a href="/overfit">Overfitting</a></li>
        
            <li><a href="/ordinary-vs-cnn">普通神经网络和卷积神经网络之间比较</a></li>
        
            <li><a href="/multinet">MultiNet学习</a></li>
        
            <li><a href="/first">第一篇博文</a></li>
        
            <li><a href="/end-to-end">end-to-end</a></li>
        
            <li><a href="/WhyDeeplearning">WhyDeepLearning</a></li>
        
            <li><a href="/DataProblem">MiniDataSet</a></li>
        
        </ul>

        <h2>Dump</h2>
        <ul class="artical-list">
        
            <li><a href="/CV%E6%8C%91%E6%88%98%E8%B5%9B">CV中非常重要的挑战赛</a></li>
        
            <li><a href="/%E5%9B%BE%E8%AE%BA%E7%AE%97%E6%B3%95">图论算法</a></li>
        
            <li><a href="/%E8%BF%91%E4%B8%A4%E5%A4%A9%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%8F%E6%84%9F%E6%83%B3">近两天对深度学习的一点感想</a></li>
        
            <li><a href="/benchmark">几个概念的理解</a></li>
        
            <li><a href="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%A6%E9%87%8F%E6%A0%87%E5%87%86">深度学习语义分割中的度量标准</a></li>
        
            <li><a href="/logits">Logits</a></li>
        
            <li><a href="/iteration%E5%92%8Cepoch">iteration和epoch比较</a></li>
        
            <li><a href="/Batch_Normalization">Batch_Normalization</a></li>
        
            <li><a href="/Memory_arrangement">linux动态内存管理</a></li>
        
            <li><a href="/ground_truth">ground truth</a></li>
        
            <li><a href="/VGG16">CNN_Architecture</a></li>
        
        </ul>

        <h2>Project</h2>
        <ul class="artical-list">
        
        </ul>
    </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>


    <script type="text/javascript">
        $(function(){
            $('.home-follow').click(function(e){
                e.preventDefault();

                if($('.home-contact').is(':visible')){
                    $('.home-contact').slideUp(100);
                }else{
                    $('.home-contact').slideDown(100);
                }
            });
        })
    </script>
</body>
</html>
