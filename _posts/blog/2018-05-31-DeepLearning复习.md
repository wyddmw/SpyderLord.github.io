---
layout: post
category: blog
title: DeepLearning复习
description: 开始暑假夏令营的准备
---
## 复习一些零碎的知识点
- 结构化和非结构化数据：意思是每个特征，都会存在一个很好的定义，比如说一个房屋大小卧室的数量，或是一个用户的年龄。相反，非结构化数据指的是音频，原始音频或者是你想要识别的图像或是文本中的内容。这里的特征可能是可能是图像中的像素值或是文本中的单词。处理非结构化的数据对于计算机来说是比较困难的，神经网络的使用使得计算机能够更好地理解非结构化的数据
- 损失函数的作用：主要是用来衡量算法的好坏。我们需要衡量输出预测值和实际值之间有多接近。而损失函数就是起到这样的一个衡量的作用。我们使用到的损失函数主要是softmax函数和svm函数。有了损失函数之后，我们就可以对我们的模型进行评估。我们在测试集上，通过最小化我们的损失函数对我们的参数w和b进行训练。那么如果对我们的损失函数进行最小化呢？使用的方法就是梯度下降的方法。我们的损失函数是一个凸函数，我们就可以使用梯度下降的方法，无论我们从什么地方开始初始化，理论上我们都可以到达同一点或是大致相同的点。之前在看CS231n的时候，对于梯度下降的讲解说的是，我们计算出偏导数之后，便可以对损失函数进行最小化的操作，沿着偏导数的方向是下降最快的方向。
![](/downloads/凸函数.png)

### 为什么深度学习会兴起
　　之前写过一篇blog[WhyDeepLearning](https://spyderlord.github.io/WhyDeeplearning)。重新看了这篇blog，看到了吴恩达的这个讲义之后，重新对这个问题有了一些认识。在讲义中，作者提到：在过去十年中，我们使用到的数据量是相对较少的，但是随着数字化进程的加快，数据量也在不断增加。传统的机器学习的算法在处理这些庞大的数据量的时候难以应对。
![](/downloads/传统机器学习算法的性能.png)
　　这个图中，横轴表示的是数据量的大小，纵轴表示的是传统机器学习算法的表现情况。可以看到，在一定的数据量的范围内，算法的表现力是随着数据量的增加而增加的，但是在一定范围之后，算法表现力的提升就处于基本停滞的状态了。这就是传统的机器学习算法不知道如何处理规模巨大的数据，在过去的十年之内，我们遇到的问题只有相对较少的数据量。<br>
　　在神经网络中展现出来的是，如果我们训练一个小型的网络，那么这个神经网络的性能可能就会出现随着数据量的不断增大然后出现在超过某个阈值之后处于停滞的状态。但是如果我们训练一个较大型的网络，网络的表现力会随着数据量的不断增加也处于不断提高的趋势。所以我们可以注意到两点：一方面我们需要训练一个规模足够大的神经网络，来发挥数据规模量巨大的优点。另一方面，我们也需要足够大的数据量。<br>
　　作者在文中提到的是：事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是要么训练一个更大的网络，要么投入更多的数据，但是在我看，这两方面也是相互影响的，就像在之前文章中写道的那样，我们需要做的不仅仅是一味的展宽网络，我们还需要加深网络的深度。