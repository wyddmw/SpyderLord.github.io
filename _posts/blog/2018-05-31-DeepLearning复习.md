---
layout: post
category: blog
title: DeepLearning复习
description: 开始暑假夏令营的准备
---
## 复习一些零碎的知识点
- 结构化和非结构化数据：意思是每个特征，都会存在一个很好的定义，比如说一个房屋大小卧室的数量，或是一个用户的年龄。相反，非结构化数据指的是音频，原始音频或者是你想要识别的图像或是文本中的内容。这里的特征可能是可能是图像中的像素值或是文本中的单词。处理非结构化的数据对于计算机来说是比较困难的，神经网络的使用使得计算机能够更好地理解非结构化的数据
- 损失函数的作用：主要是用来衡量算法的好坏。我们需要衡量输出预测值和实际值之间有多接近。而损失函数就是起到这样的一个衡量的作用。我们使用到的损失函数主要是softmax函数和svm函数。有了损失函数之后，我们就可以对我们的模型进行评估。我们在测试集上，通过最小化我们的损失函数对我们的参数w和b进行训练。那么如果对我们的损失函数进行最小化呢？使用的方法就是梯度下降的方法。我们的损失函数是一个凸函数，我们就可以使用梯度下降的方法，无论我们从什么地方开始初始化，理论上我们都可以到达同一点或是大致相同的点。之前在看CS231n的时候，对于梯度下降的讲解说的是，我们计算出偏导数之后，便可以对损失函数进行最小化的操作，沿着偏导数的方向是下降最快的方向。
![](/downloads/凸函数.png)
- 梯度下降(gradient descent): 首先来回顾一下梯度下降的作用是什么——在我们的测试集上，通过最小化代价函数（损失函数）的方式来对我们的参数进行训练。通常我们的损失函数是有两个参数的，一个是权值矩阵W，另一个是偏差矩阵b，所以我们在计算偏导数的时候也是对于这两个参数来分别计算的。
- 逻辑回归(Logistic Regression):对于一个二分类的问题来说，给定一个输入特征向量，这个输入特这个你可能是一个图像或是其他的图片。我们想要一个输出y'来表示对实际值y的估计。我们使用参数w来计算逻辑回归的参数，除了w之外，我们还需要一个偏差b参数。我们不能直接对输入进行线性的计算W*X+b来表示我们最后的输出，因为最后的输出结果范围可能会超过1甚至是成为负数。但是我们最初希望的是通过输出的预测来表示一个概率的估计，这显然是不符合我们预先的设想的。在这个线性的计算之后我们需要增加一层非线性函数——sigmoid函数，将输出压缩到0～1之间，用这个输出的结果来表示预测的概率。为什么树上说的逻辑回归只能用于二分类，虽然我应该知道他想表示的意思应该是在现有的所有数据中，通过一条线将现有的数据分隔开。但是很显然我们通过对矩阵的尺寸的调整就可以实现多个分类的问题，使用非线性函数对输出的结果进行压缩使输出看起来更像是概率。

### 为什么深度学习会兴起
　　之前写过一篇blog[WhyDeepLearning](https://spyderlord.github.io/WhyDeeplearning)。重新看了这篇blog，看到了吴恩达的这个讲义之后，重新对这个问题有了一些认识。在讲义中，作者提到：在过去十年中，我们使用到的数据量是相对较少的，但是随着数字化进程的加快，数据量也在不断增加。传统的机器学习的算法在处理这些庞大的数据量的时候难以应对。
![](/downloads/传统机器学习算法的性能.png)
　　这个图中，横轴表示的是数据量的大小，纵轴表示的是传统机器学习算法的表现情况。可以看到，在一定的数据量的范围内，算法的表现力是随着数据量的增加而增加的，但是在一定范围之后，算法表现力的提升就处于基本停滞的状态了。这就是传统的机器学习算法不知道如何处理规模巨大的数据，在过去的十年之内，我们遇到的问题只有相对较少的数据量。<br>
　　在神经网络中展现出来的是，如果我们训练一个小型的网络，那么这个神经网络的性能可能就会出现随着数据量的不断增大然后出现在超过某个阈值之后处于停滞的状态。但是如果我们训练一个较大型的网络，网络的表现力会随着数据量的不断增加也处于不断提高的趋势。所以我们可以注意到两点：一方面我们需要训练一个规模足够大的神经网络，来发挥数据规模量巨大的优点。另一方面，我们也需要足够大的数据量。<br>
　　作者在文中提到的是：事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是要么训练一个更大的网络，要么投入更多的数据，但是在我看，这两方面也是相互影响的，就像在之前文章中写道的那样，我们需要做的不仅仅是一味的展宽网络，我们还需要加深网络的深度。