---
layout: post
category: blog
title: MiniDataSet
description: DL遇到小批量的数据集
---

## 如果我们现有的数据集很少，是不是深度学习就难以适用？
- 还是和研究生学姐一起完成的的任务，现在面临的问题是，我们手边中的数据集有限，没有办法进行大量的训练，在少量样本的前提下我们是不是应该抛弃深度学习的方法，转向传统的分类方法？一直存在着这样的声音：深度学习如果没有大量的数据来驱动，性能是并不理想的。在去北京听讲座的时候，老师也提到了这个问题，数据集是当前限制AI发展的一个瓶颈吧，毕竟带有标签封装好的数据集确实是非常有限的。然而问题就在于现实生活中，我们很少能够有机会获得像cifar-10或是mnist这样庞大的数据集，我们只能自己采集，但是在医学领域，这显然是不太现实的！
- 从现实的角度，我们知道用大数据做驱动虽然好用但是不方便使用。在理论的角度，我想不明白一个问题，像CNN这样的网络，它可以说是一种工具吧，难道在需要工具去解决某个特定的问题的时候，对这个问题还要有有限定才能进行解决么．如果我们拿着螺丝刀去修车，还必须要车子的数量非常庞大才能将螺丝刀用上去么？我觉得这里存在着一个误区．
- 经过一段时间的学习之后，拐回头再来看这个问题．在小数据集的情况下我们也是依然能够使用深度学习的方法进行学习的．首先可以通过data-argumentation的方法，对现有的数据进行一系列的旋转等预处理，这样可以使训练出来的模型更加robust．其次，现在的一种操作是直接使用别人训练好的模型，经过这么多年的世界挑战赛的推动，现有的模型和已经训练好的参数集已经能够对应出非常多类别下的classification，这对于像常见物体的识别分类来说，我们是不需要非常庞大的数据集的，直接使用预训练好的模型就可以了．在multinet对应的论文中，Marvin等人只使用了2００多张图片便得到了非常好的训练效果，他们是使用的vgg16的网络结构，所以，随着DL的发展，我觉得数据的这个瓶颈一定会被解决的，我觉得更多的可能是通过更加efficient模型的提出．