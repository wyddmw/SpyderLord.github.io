---
layout: post
category: blog
title: 大数据和机器学习学习笔记
description: 看一本白话大数据和机器学习的书，做一些笔记
---

## 补充一些相关的概念
- 古典概型：也叫传统概率，如果一个随机实验所包含的单位事件是有限的，且每个单位事件发生的可能性均相等，那么这个随机实验就叫做拉普拉斯试验，这种条件下的概率模型就叫做古典概型。判断一个试验是否为古典概型，在于要判断这个试验是否具有古典概型的两个特征——有限性和等可能性，只有同时具备这两个特点的概型才是古典概型。
- 几何概型：一种概率模型，在这个模型下，随机实验所有可能的结果是无限的，并且每一个基本结果发生的概率是相同的。比如一个人到单位的时间在某个时间区间内等。判断一个试验是否为几何概型在于这个试验是否具有几何概型的两个特征——无限性，等可能性。几何概型和古典概型是相对的。
- 概率这个概念：概率这个定义是通过统计得到的。在抛硬币的场景中，我们说的概率实际上是通过统计得出的，而并非是因为硬币存在两个面，概率本身是对大量样本分布比例的一种解释，而不是对单一一次事件的可能性的解释。

## 信息论相关
　　信息最为广泛接受的一种定义是——信息是倍消除的不确定性。
- 信息量：在信息论中，对信息量是有确定解释并且是可以量化计算的，这里提到的信息量是一种信息数量化度量的规则。具体的补充待看了通信原理之后再进行更加详细的补充。
- 我们在计算信息量的时候，使用的概率是先验概率。
- 香农公式：在通信工程中，存在一个非常重要的公式就是香农公式。C=B log2(1+S/N)。其中B表示码元速率，S/N表示信噪比，传输的信号功率和在这个信道中产生的各种信号噪声的功率的大小比值。这个公式说明了信号传输速率和信噪比以及带宽之间的关系。从公式来看，带宽越大传输的速率就越大。
- 信息熵：它可以认为是用来描述信息的杂乱程度的。看完之后，我认为信息熵这个概念是用来对整个事件进行描述的，该事件发生可能会有多种不同的情况，每一种情况对应的会有一个信息量，而信息熵就是对各个不同的情况的信息量的数学期望，对整个事件进行一个整体上的描述。

## 多维向量空间
- 向量的概念：具有大小和方向的几何对象。
- 维度：维度指的是参照系，有多少个维度就有多少个参照系，维度的设定一般具有正交性的。

## 分割线，从下面开始，就要开始接触相关的机器学习的相关内容了。

## 回归分析
- 线性回归：线性回归是利用数理统计学中的回归分析来确定两种或两种以上变量之间相互依赖的定量关系的一种统计分析方法。重点在于——线性回归是用来确定一种定量关系的，这是重点所在。
- 线性回归中使用的是残差分析，残差在数理统计中是指实际观察值和估计值（拟合值）之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话，我们可以将残差看做误差的一个观测值。在残差网络中，我们将中间的hidden layer的功能看做是残差，通过网络的优化，将这个残差不断缩小，进而可以实现identity match的效果。回到我们的回归分析中来。在线性回归中，我们希望最终能够得到一个函数来表示y和x之间的关系。一种非常经典的用来进行线性回归中的系数猜测的方法是——最小二乘法，我们希望得到的这样的一个函数关系，预测值和实际值之间的差值最小。我们假设有多个x、y的样本值，我们尝试用一个函数来进行拟合这种关系，残差的表示就是e=ax+b+-y，然后对e计算绝对值。误差的大小是预测值ax+b和观测值y之间的差值。我们尝试讲所有的差值进行求和，构造出来一个函数，下面的问题就转换为如何将这个残差函数值最小化。写到这里就可以和梯度下降结合起来了，残差和的函数分别对a和b进行求导，然后对a和b分别进行调整，最终使得残差的和最小，实现理想的拟合过程，通过梯度下降的方式对a和b的值进行调整。写到这里，想起来之前和研究生讨论过的一个问题，他去听讲座的时候，主讲人讲的是目前常用的训练的优化的方法或是说损失函数本质上都是最小二乘法。现在仔细想想确实是这样的，通过最小二乘法进行优化，通过最小化误差的平方和寻找数据的最佳函数匹配。

## 分类问题
　　在之前专门写的一个博客中有写到，分类问题的输出结果是一个离散的值，和回归问题给出的是一个连续的值是不同的。分类算法一类大的算法，都是用来解决这种离散变量预测值的。

### 贝叶斯分析
 　　朴素贝叶斯：其实之前在看西瓜书或是看其他相关机器学习算法的时候，就有看到过介绍过贝叶斯分析的。贝叶斯分析是非常牛逼的算法，之前学习概率论的时候有简单学习过贝叶斯分析，在没有看书的前提下回想这个贝叶斯分析，我所能想起来的就是这个算法是利用先验概率和概率密度去推算后验概率密度的。
- 贝叶斯决策理论方法：1.已知类条件概率密度参数表达式和先验概率。2.利用贝叶斯公式转换成后验概率。3.根据后验概率大小进行决策分类。<br>
　　简单来说，朴素贝叶斯公式就是利用统计中的“条件概率”来进行分类的一种算法。贝叶斯概率研究的是条件概率，也就是研究的场景是在带有某些场景的前提条件下，或者在某些背景条件的约束下发生的概率问题。
- 贝叶斯公式：P(A∩B) = P(A)×P(B｜A)=P(B)×(A｜B)。贝叶斯公式一般简写为：P(A｜B)×P(B)=P(B｜A)×P(A)。对应变量的意义如下:
- P(A):事件A的先验概率，就是一般情况下事件A发生的概率。P(B｜A)的叫做似然概率，是A假设条件成立的情况下发生B的概率。P(A｜B)叫做后验概率，在事件B发生的情况下发生A的概率，也就是要计算的概率。P(B)叫做标准化常量，和A的先验概率是定义类似，就是一般情况下，事件B发生的概率。所以使用贝叶斯分析的思路就是使用现有的数据进行建模处理，可能我需要结合代码才能更加明确这里所说的建模是什么样的一个概念或是操作。从目前自己的理解来看就是使用现有的数据集去计算相关的各个概率值，然后根据计算出来的各个先验概率以及相关的似然概率等去得到最后我们想要的后验概率。

### SVM支持向量机
　　摘抄之前写的博客中的一点内容，作为一点回顾。SVM本身对应的是一个二分类的问题，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略是间隔最大化，最终可以转换成为一个凸二次规划问题的求解。一个线性分类器的学习目标就是在n维的数据空间中找到一个超平面，这个超平面的方程表示为W(T)X+b=0。距离超平面最近的几个训练样本点使符号函数g(z)=sign(x)成立，这些样本点称为是支持向量。然后我们需要做的是异类支持向量到超平面的距离最大化——可以和SVM的损失函数结合起来。
- 关于超平面的一点说明：超平面是一个抽象的概念，在一维空间中就是一个点，用x+A=0来表示；在二维空间中就是一条线，用Ax+By+c=0来表示，Ax+By+c>0表示的是一类，>0的表示为一类；在三维空间中就是一个面，用Ax+By+Cz+d=0来表示等。
- 距离的问题：在回归问题中，我们希望拟合出来一条曲线能够和得到的样本数据点的距离最近，在SVM支持向量机中，我们希望让被分开的两个类别的距离尽可能远，也就是相差越远分的越开。在平面直角坐标系中点到直线的距离表示为d=｜Ax+By+c｜/sqrt(A^2+B^2)。超平面的表达式实际上可以简写成g(v)=wv+b。其中v是样本向量，b是常数。在二维的空间中，v就是(x,y)，三维空间中就是(x,y,z)，其余的情况是类似的，而w也是一个向量，在二维空间中就是(A,B)，在三维空间中就是(A,B,C)。
- 有点不太想继续写下去了，做一个简单的小结：SVM解决问题的方法描述起来对应的是下面的步骤：
1. 将所有的样本和其对应的分类标记交给算法进行训练。
2. 如果线性可分，就直接找出超平面
3. 如果线性不可分，就将数据映射到n+1高维空间，找出超平面
4. 最后得到超平面的表达式，也就是分类的函数。