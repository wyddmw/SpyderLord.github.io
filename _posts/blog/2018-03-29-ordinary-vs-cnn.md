---
layout: post
category: blog
title: 普通神经网络和卷积神经网络之间比较
description: 我们为什么现在更愿意使用卷积神经网络
---

## 我们为什么要使用CNN而不是之前普通的神经网络。  
　　这两天和一个研究生学姐一起完成她的一个关于帕金森语音诊断的语音识别的小项目，因为是医学方面的数据集，所以数据量有限，只有几百个数据，然后每个数据的尺寸也不大。学姐的思路非常有创新性，她将语音信号转换成了图片的形式（虽然我也没有看到数据集到底是什么样子的，因为还没有征得老师的同意，医学上的数据集不方便外传，但是我之后想了一下应该就是转换成了一个二维矩阵的形式）。她用结构是最简单的LeNet的结构，在她已有的结构下，她在测试集上得到的准确率达到了90%的，她觉得可能是数据量有限，出现了过拟合的现象。 
后来我想，在现阶段，有很多非常成熟的层的结构，为什么不使用现有的更好的结构进行训练呢（比如是AlexNet、GoogleNet等），然后我又去看了一下VGGNet，发现了一个问题，不管是VGG还是AlexNet，两个结构的输入都是227*227*3的尺寸，数据量都是非常大的。猛地想起来在介绍卷积的时候，CNN是更适用于大尺寸的输入，能够有效的减少参数的个数，那么问题来了：
## 在对于小尺寸的输入的时候我们是不是不应该使用CNN？又一想，不对啊，cifar-10的输入的尺寸也不大啊，但是也用CNN去训练了。
## 发现自己好像还是没有发现普通神经网络和卷积神经网络直接最本质的区别所在。然后上了知乎，看了一些人的回答之后，感觉多少有一些明白了，现在对看到的回答做一个简单的总结。

- CNN本身要针对的图像数据恰好有这个特点，然后就根据局部感受野这个性质提出的局部权值共享。CNN能够捕获图像的局部特征，所以可以很好的用来进行图片的分类，这些好的性质都来自于卷积的操作。在别的应用领域，比如语音信号和自然语言处理问题中，CNN都有比较理想的效果。
- CNN的作用有点像自然语言处理中的二元语法，也就是认为图片中的卷积核尺寸大小的数据都是相关的，所以所以认为某个像素和其他像素多少都有一些关系，并把这些关系作为特征。
- 如果将机器对图片的识别和人对图片的识别相类比，普通的神经网络就相当于是在一个像素一个像素的学习，但是卷积神经网络却是在一个局部一个局部的学习图片的特征。
- 一个卷积核就相当于是在提取一个图像的一种特征，所以就相当于是使用多个卷积核就能够提取到多个特征，所以也有文章说道就是CNN能够提取到多个局部特征。
- 所以说前面的卷积层是在提取局部特征，在最后的全连接层，得到的是全局的一个特征，需要注意的是卷积核的个数不能太多，太多的话可能就会造成提取到一些无用的特征。就好比我们在训练人脸识别的分类器，如果我们的卷积核的个数非常多，很有可能机器将人脸上的青春痘都当成了一种特征进而提取出来，这很明显是不正确的做法了．


