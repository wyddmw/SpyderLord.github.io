---
layout: post
category: blog
title: 普通神经网络和卷积神经网络之间比较
description: 我们为什么现在更愿意使用卷积神经网络
---

## 我们为什么要使用CNN而不是之前普通的神经网络。  
　　这两天和一个研究生学姐一起完成她的一个关于帕金森语音诊断的语音识别的小项目，因为是医学方面的数据集，所以数据量有限，只有几百个数据，然后每个数据的尺寸也不大。学姐的思路非常有创新性，她将语音信号转换成了图片的形式（虽然我也没有看到数据集到底是什么样子的，因为还没有征得老师的同意，医学上的数据集不方便外传，但是我之后想了一下应该就是转换成了一个二维矩阵的形式）。她用结构是最简单的LeNet的结构，在她已有的结构下，她在测试集上得到的准确率达到了90%的，她觉得可能是数据量有限，出现了过拟合的现象。 
后来我想，在现阶段，有很多非常成熟的层的结构，为什么不使用现有的更好的结构进行训练呢（比如是AlexNet、GoogleNet等），然后我又去看了一下VGGNet，发现了一个问题，不管是VGG还是AlexNet，两个结构的输入都是227×227×3的尺寸，数据量都是非常大的。猛地想起来在介绍卷积的时候，CNN是更适用于大尺寸的输入，能够有效的减少参数的个数，那么问题来了：
## 在对于小尺寸的输入的时候我们是不是不应该使用CNN？又一想，不对啊，cifar-10的输入的尺寸也不大啊，但是也用CNN去训练了。
## 发现自己好像还是没有发现普通神经网络和卷积神经网络直接最本质的区别所在。然后上了知乎，看了一些人的回答之后，感觉多少有一些明白了，现在对看到的回答做一个简单的总结。

- CNN本身要针对的图像数据恰好有这个特点，然后就根据局部感受野这个性质提出的局部权值共享。CNN能够捕获图像的局部特征，所以可以很好的用来进行图片的分类，这些好的性质都来自于卷积的操作。在别的应用领域，比如语音信号和自然语言处理问题中，CNN都有比较理想的效果。
- CNN的作用有点像自然语言处理中的二元语法，也就是认为图片中的卷积核尺寸大小的数据都是相关的，所以所以认为某个像素和其他像素多少都有一些关系，并把这些关系作为特征。
- 如果将机器对图片的识别和人对图片的识别相类比，普通的神经网络就相当于是在一个像素一个像素的学习，但是卷积神经网络却是在一个局部一个局部的学习图片的特征。
- 一个卷积核就相当于是在提取一个图像的一种特征，所以就相当于是使用多个卷积核就能够提取到多个特征，所以也有文章说道就是CNN能够提取到多个局部特征。
- 所以说前面的卷积层是在提取局部特征，在最后的全连接层，得到的是全局的一个特征，需要注意的是卷积核的个数不能太多，太多的话可能就会造成提取到一些无用的特征。就好比我们在训练人脸识别的分类器，如果我们的卷积核的个数非常多，很有可能机器将人脸上的青春痘都当成了一种特征进而提取出来，这很明显是不正确的做法了．

## 2018-08-11分割线
　　重新拾起之前看的卷积神经网络，我们使用卷积网络的原因上面也写到了，卷及神经网络更加适合这种具有结构层次性的输入上，类似图片这样的输入，是一个三维的输入，我们使用的卷积核也是一个三维卷积核，我们使用卷及神经网络的时候，处理的是一个局部的信息，忘记在什么地方上看到这样的结论：卷积神经网络实际上是更适合用于解决类似图像识别这样的问题的，因为实际上我们人在看某个物体的时候也是从局部开始看的，最后得到全局的一个判断，这和卷积神经网络在某种程度上契合的。常规的神经网络每一个神经元都和前面所有输入有联系，也就是全连接的概念，但是卷积神经网络的神经元之和输入的一部分区域相连接。卷积神经网络更适合用于大尺寸的输入。<br>
　　卷积核的数量越多，可以理解为我们拟合出来的函数是越复杂的，很容易在训练集上产生过拟合的结果。<br>
　　汇聚层的作用是进行降采样的，对输入数据空间进行降采样，这里需要注意的是我们使用的感受也不能太大，否则会造成信息的大量丢失，通常使用2*2的感受野就可以了。