---
layout: post
category: blog
title: DeepLearning(四)
description: 随机梯度下降以及各种参数更新的方法，这个是一直没有仔细好好看的内容
---

## 普通更新
　　最简单的梯度下降的参数更新的方法，沿着梯度的反方向进行参数的更新，梯度的方向是上升的方向，我们需要的是最小化损失函数，通常我们都会设置好学习率，是一个固定的值，当然如果我们使用学习率退火的方式的话，我们是需要不断调整学习率的常数值的。只要学习率足够低，我们总是能够在损失函数上得到非负的进展。

## 动量更新的方法Momentum
　　这种方法在深度网络上几乎总能得到更好的收敛速度。从物理的角度出发进而得到相应的启发。传统的参数更新的方法是从位置的角度来进行的，这次更新完成之后看是否到达了预期的位置，如果没有到达的话继续向目标位置前进。采用动量来进行参数的更新，更新的是速度，也就是这个点没有到达最优值，那就沿着负梯度的方向再走快点试试，然后再更新位置状态。知乎上的一个回答是这样的：超参数mu是为了保证系统最后实现收敛到局部值，比如说我们现在即将到达局部最小点上了，因此速度更新量越来越小，因为梯度已经接近于零，但是有速度就会继续前进，因为v不会因为dx接近于零就等于零了，在mu超参数的作用下会再慢慢减小到零，这样有助于到达最终我们期望的收敛点的位置。<br>
　　通俗的说，我们从山腰开始向下走，第一步0.5米，第二步0.6米，但是此时我们距离山脚还很远，所以我们就借着上一步的冲劲在第二步的时候走了可能1.1米，然后第三步又在第二步的基础上可能走了1.15米，加入动量之后，可以非常大程度的实现参数的更新。