---
layout: post
category: blog
title: ResNet网络的学习
description:
---

## 什么是残差
　　残差在数理统计中是指实际观察值与估计值(拟合值)之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话，我们可以将残差看作误差的观测值。它应符合模型的假设条件，并且具有误差的一定的性质。利用残差所提供的信息，来考察模型假设的合理性和数据的可靠性称为残差分析。(初步读完paper，整个网络应该是对这个残差进行调优)<br>
## 学术界的霸主——ResNet
　　之前看了AlexNet、VGG16和VGG19等相关的网络结构，VGG16是在12年的时候提出来的，在很长一段时间内都是占据着主导地位的。在15年的时候，微软研究院提出的残差网络取代VGG16网络结构，成为学术界的霸主。LeNet、AlexNet以及VGGNet在内都是比较古老的网络结构了，因为VGG等相关网络的结构简单，工业界对这些网络都有非常有效的加速优化，但是因为缺乏泛化能力在学术界逐渐被淘汰。首先来看之前VGG等传统卷积网络存在的问题。
- 第一个问题就是在网络越深，学习越难——vanishing/exploding gradients，梯度弥散或是梯度爆炸，这在一开始阻碍了收敛的进行。而且越复杂的网络越容易产生过拟合的问题。但是随着一些规则化方法的使用比如BN使得网络能够收敛。
![](/downloads/converging.png)
- 但是在网络能够收敛之后，出现的另一个问题是degradation:![](/downloads/degradation.png)
随着网络深度的加深，训练的精度开始停滞，然后迅速下降,这个问题不是因为overfitting产生的，因为overfitting的情况下，在训练集上的正确率是很高的，但是在这里，训练集上的正确率也出现了下降的情况(20层和56层的网络相比)——直接说说明了如果不加修改只是单纯的堆网络的话，其实效果是不好的。
![](/downloads/fig1.png)
　　提出ResNet的根本目的就是为了解决在网络深度增加的时候出现的degradation现象。paper中指出，作者等人通过建立深度的残差网络来实现解决degradation的问题。Introduce a deep residual learning framework.我们可以设想一个浅层网络和它对应的深层的网络，存在一个更深的模型进行构建，增加的层是identity mapping(恒等映射，输入为X，经历多个非线性网络之后输出的仍然是X)，然后其他网络是从已学的浅层模型中复制。<br>
　　如果原来期望的潜在映射是H(x),堆积的函数非线性层匹配另一个映射F(x)=H(x)-x,这个F(x)实现的就是一个残差函数，我们在最优化的过程中，可以看作是对这个残差函数进行最优化，不断的让残差函数逼近0,实现使堆叠层网络逼近恒等映射。
![](/downloads/ResNet1.png)
![](/downloads/ResNet2.png)
　　衰退问题表明，求解程序在通过多个非线性网络层去逼近恒等映射的时候是存在困难的。由于残差学习的重新公式化表明，如果恒等映射是最优的，求解程序也许只要简单地将多个非线性网络层的权重向0逼近，以此来逼近恒等映射。也就是上面打的，其实多个堆叠网络的功能实现的是残差函数，然后对残差函数进行optimization操作，最终实现添加的H(x)逼近identity mapppings—— 应该这个就是原理了。

### 重点就在上面的这段，如果我们添加的层是恒等映射层identity mappings，那么深层网络的效果不会比对应的浅层网络的效果差。因此，整个堆叠的网络层起到的作用就相当于是恒等映射的功能。
　　在现实情况下，实现恒等映射是不太可能的，但是重新化公式也许有助于预处理网络衰退的问题。

## 一个重要的概念
- Shortcut Connections:Shortctut Connections are those skipping one or more layers.In this case, the shortcut connections simply perform identity mapping,and their outputs are added to the outputs of the stacked layers.Identity mappings add no extra parameter nor computational complexity.配合着block的框图也许可以更好的理解：
![](/downloads/block.png)

## ResNet的网络结构：
![](/downloads/ResArch1.png)
![](/downloads/ResArch2.png)
![](/downloads/ResArch3.png)
![](/downloads/ResArch4.png)

## 关于ResNet的性能的问题
　　上面提到了，identity mappings其实是没有增加参数和额外的计算复杂度的。即使网络的深度达到了152层，整个网络的复杂程度也不及VGG19，充分利用了深层次的网络。Papaer中在其中一个部分写道：
![](/downloads/Res152.png)

## 分割线 2018-04-27
- projection shortcut这个概念在第一次看paper的时候没有怎么注意这个问题，在看程序的时候发现了这个名词的出现，看程序的时候不太理解这个是什么意思，再看paper的时候，在文中找到了这个名词的出现。Projection的出现是为了解决输入X和F(x)维度不相等的问题。作者给除了两种解决的方案：一个是identity mapping 通过给低维度的x加0来增加维度，这种方式不会增加多余的参数——parameter-free，另一种就是projection shortcut给x乘上一个投影矩阵。实验表明：identity mapping is sufficient for addressing the degradation problem and is ecomocial,and thus Ws is only used when matching dimensions.
- bottleneck architecture.这个结构是在网络进一步加深的情况下执行的。附上paper中对这部分的描述：
![](/downloads/bottleneck.png)
　　作者说的是出于对运算时间的考虑，将表示残差函数对应的非线性函数的(stack of 2 layers)替换成了3层的。添加了1×1的卷积核，这样做的目的是为了在输入的时候降低维度，然后在输出的时候增大维度，使中间的3×3的层都有一个较小的输入输出维度。如果我们重新看网络结构的话，我们可以发现确实这样，如果不把维度降下去，多个3×3的卷积层会有非常多的参数，计算起来确实复杂程度还是可观的。所以我觉得，作者应该是出于这样的考虑然后对网络的结构进行了修改吧。<br>
　　作者在这段的下面又提了一句，如果在x和F(x)维度不相等的时候，将identity mapping 替换成 projection shortcut，不管是参数的数量还是运算的复杂程度上都会增加的非常多！
- 附上在网上找到的一个在更加深层情况下的bottleneck单个block的结构示意图，对于理解MultiNet也是非常有帮助的。
![](/downloads/Res50.png)