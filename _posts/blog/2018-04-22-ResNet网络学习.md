---
layout: post
category: blog
title: ResNet网络的学习
description:
---

## 什么是残差
　　残差在数理统计中是指实际观察值与估计值(拟合值)之间的差。“残差”蕴含了有关模型基本假设的重要信息。如果回归模型正确的话，我们可以将残差看作误差的观测值。它应符合模型的假设条件，并且具有误差的一定的性质。利用残差所提供的信息，来考察模型假设的合理性和数据的可靠性称为残差分析。(初步读完paper，整个网络应该是对这个残差进行调优)<br>
## 学术界的霸主——ResNet
　　之前看了AlexNet、VGG16和VGG19等相关的网络结构，VGG16是在12年的时候提出来的，在很长一段时间内都是占据着主导地位的。在15年的时候，微软研究院提出的残差网络取代VGG16网络结构，成为学术界的霸主。LeNet、AlexNet以及VGGNet在内都是比较古老的网络结构了，因为VGG等相关网络的结构简单，工业界对这些网络都有非常有效的加速优化，但是因为缺乏泛化能力在学术界逐渐被淘汰。首先来看之前VGG等传统卷积网络存在的问题。
- 第一个问题就是在网络越深，学习越难——vanishing/exploding gradients，梯度弥散或是梯度爆炸，这在一开始阻碍了收敛的进行。而且越复杂的网络越容易产生过拟合的问题。但是随着一些规则化方法的使用比如BN使得网络能够收敛。
![](/downloads/converging.png)
- 但是在网络能够收敛之后，出现的另一个问题是degradation:![](/downloads/degradation.png)
随着网络深度的加深，训练的精度开始停滞，然后迅速下降,这个问题不是因为overfitting产生的，因为overfitting的情况下，在训练集上的正确率是很高的，但是在这里，训练集上的正确率也出现了下降的情况(20层和56层的网络相比)——直接说说明了如果不加修改只是单纯的堆网络的话，其实效果是不好的。
![](/downloads/fig1.png)
　　提出ResNet的根本目的就是为了解决在网络深度增加的时候出现的degradation现象。paper中指出，作者等人通过建立深度的残差网络来实现解决degradation的问题。Introduce a deep residual learning framework.我们可以设想一个浅层网络和它对应的深层的网络，存在一个更深的模型进行构建，增加的层是identity mapping(恒等映射，输入为X，经历多个非线性网络之后输出的仍然是X)，然后其他网络是从已学的浅层模型中复制。<br>
　　如果原来期望的潜在映射是H(x),堆积的函数非线性层匹配另一个映射F(x)=H(x)-x,这个F(x)实现的就是一个残差函数，我们在最优化的过程中，可以看作是对这个残差函数进行最优化，不断的让残差函数逼近0,实现使堆叠层网络逼近恒等映射。
![](/downloads/ResNet1.png)
![](/downloads/ResNet2.png)
　　衰退问题表明，求解程序在通过多个非线性网络层去逼近恒等映射的时候是存在困难的。由于残差学习的重新公式化表明，如果恒等映射是最优的，求解程序也许只要简单地将多个非线性网络层的权重向0逼近，以此来逼近恒等映射。也就是上面打的，其实多个堆叠网络的功能实现的是残差函数，然后对残差函数进行optimization操作，最终实现添加的H(x)逼近identity mapppings—— 应该这个就是原理了。
### 重点就在上面的这段，如果我们添加的层是恒等映射层identity mappings，那么深层网络的效果不会比对应的浅层网络的效果差。因此，整个堆叠的网络层起到的作用就相当于是恒等映射的功能。
　　在现实情况下，实现恒等映射是不太可能的，但是重新化公式也许有助于预处理网络衰退的问题。

## 一个重要的概念
- Shortcut Connections:Shortctut Connections are those skipping one or more layers.In this case, the shortcut connections simply perform identity mapping,and their outputs are added to the outputs of the stacked layers.Identity mappings add no extra parameter nor computational complexity.配合着block的框图也许可以更好的理解：
![](/downloads/block.png)

## ResNet的网络结构：
![](/downloads/ResArch1.png)
![](/downloads/ResArch2.png)
![](/downloads/ResArch3.png)
![](/downloads/ResArch4.png)

## 关于ResNet的性能的问题
　　上面提到了，identity mappings其实是没有增加参数和额外的计算复杂度的。即使网络的深度达到了152层，整个网络的复杂程度也不及VGG19，充分利用了深层次的网络。Papaer中在其中一个部分写道：
![](/downloads/Res152.png)