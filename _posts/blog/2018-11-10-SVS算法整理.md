---
layout: post
category: blog
title: SVS算法整理
description: 先重点整理一下stereo matching部分的程序
---

## 概述
　　现有的基于深度学习的单目深度估计方法，通常是将CNN网络作为一个黑盒子来使用，学习图像块到深度值的直接映射，这类方法完全依赖高级语义信息作为预测深度的依据，尽管有些方法在损失函数上引入了一些特殊的约束条件，学习这样的语义信息仍然是非常困难的。另一方面，即使这样的映射能够被成功训练出来，算法可能需要非常大量的带有深度标签值的真实数
据，对数据的采集提出了很高的要求，也限制了这类技术的适用场景。整个网络将双目深度估计分解成为两个部分来完成，第一个部分是视觉合成网络——View Synthesis Network，第二个部分是双目匹配模型——Stereo Matching Network。这样的分解带来了两个显著的优势，一方面是极大减少了深度标注数据的依赖，在测试阶段，显式地引入了几何约束。<br>
　　关于这个模型的精度问题，首先是超过了Kitti上所有单目深度估计方法，并首次依赖单目深度图像数据就超过了双目匹配算法Block Matching的深度估计。<br>
　　模型的视图合成过程由视图合成网完成，输入一张左图，网络合成该图像对应的右图；而双目匹配过程由双目匹配网络完成，接收左图和合成的右图，预测出左图每一个像素的视差值。![/downloads/SVS.png](/downloads/SVS.png){:height="185" width="360"}

## Synthesis Part
　　对于一个双目对来说，双目视图由一个同步的、经校正的相机生成，致使像素在水平方向上有很强的相关性。不像之前的warpbased的方法，大体上要求对其中的几何学有一个精准的评估，Deep3d提出了一个新的概率模式来从初始的图像来转换像素。这样即可通过一个可微的选择层，实现从左目图像到右目图像的转换。我们借鉴了其设计理念，并基于此来建立我们的视觉合成网络。<br>
　　训练所用的数据来着KITTI 2015，以图像对为单位，每个图像对包含左侧相机和右侧相机对同一场景所拍摄的图像。用于训练的网络有67个输入：<br>
　　1. 一个原始的左目视图Data(640×192×3)一个右目视图Label(640×192×3)<br>
　　2. 65个备用图Dis_N(N∈[0, 64])，其相对于原始左目视图，在水平方向上有不同程度位移，用于与最后的概率矩阵结合回归出最终结果。<br>
　　该网络的一次前向计算大致为:<br>
　　1. 一个左目图像经过一个基础网络(VGG 16)进行处理, 经过全连接层和Reshape之后输出维度为(20×6×128)<br>
　　2. 从来自不同深度的层采集到的特征上采样到相同的分辨率(320×96×128)<br>
　　3. 经过上采样后的特征图，经过累加、逆卷积、卷积生成一个视差概率图(640×192×65)<br>
　　4. 通过Softmax得到对应的65个通道的概率图<br>
　　5. 通过一个选择模块，将Dis_N 与对应的概率图进行加权求和，得到合成图:
![/downloads/synthesis.png](/downloads/synthesis.png){:height="320" height="200"}

### 实现细节
　　VGG16 将作为该模型的基网络，初始的参数来自于其在ImagNet上的训练结果。在Deep3D的基础上，我们做了一些改动来适应KITTI 2015的训练数据：<br>
　　1. 输入数据的分辨率为640×192<br>
　　2. 在每个对输出的feature map 的逆卷积运算之前，又加上一个卷积层<br>
　　3. 更改了视差的变化范围(从原本的33增加到65)<br>
　　4. 为了适应更大的输入和更深的网络结构，Batchsize被减小为2，并去掉了Deep3D原有的BatchNorm层。<br>
　　在每个pooling层之后，有一个分支对feature map进行一次逆卷积，相当于对其进行一次上采样，输出的feature map维度是(320×96×128)<br>
　　来自各个逆卷积层的feature maps 将累加起来处理得到与输入尺寸相同的特征表示。并应用softmax, 得到对应各个通道（65个）相对于不同视差程度下的概率矩阵。通过Slice操作提取出这65个概率图，用于与来自输入的65个不同的视差图进行点乘和累加运算，得到最终的右目视觉合成图。

### 损失函数
　　损失函数在此没有被特别定义，L1 loss用来监督图像重建对于视觉合成任务来说已经足够，Lr表示被合成的右侧图像。![/downloads/loss_3.png](/downloads/loss_3.png){:height="40" width="200"}
　　选择的过程是该网络的一个核心部分。该部分不同于以往的的基于深度图像生成（Depth Image-Based Rendering）的技术，DIBR 直接通过估计到的深度信息来直接将左图变形得到右侧图像，且该方法需要一个精度较高的深度图，同时该方法不是一个完全可微的操作，因此限制了其在深度学习上的实用性。<br>
　　而本文中的选择模型将图像重建定义为一个概率性的累加过程：![/downloads/loss_4.png](/downloads/loss_4.png){:height="30" width="130"}
　　D作为一个概率视差结果，其维度为W×H×C，分别代表图像长，宽，C代表用于选择器进行累加的可能的视差值的选项数，代表不同的视差通道，此处d即为C。![/downloads/format.png](/downloads/format.png){:height="30" width="130"}
　　Il^d表示左侧图像对应的不同视差图，公式表示右侧图像的像素预测值是来自左侧对应位置的像素及其在一定的水平位移范围内的像素的加权和，其权重来自视差矩阵。该操作将所有的65的输入通过学习到的权重累加起来，保留了全局的可微性。


## Stereo_Matching Part

### FlowNet && FlowNetC
　　说起来DispNet和DispNetC网络，它们又是建立在FlowNet和FlowNetC的基础上，下面主要介绍一些FlowNet和FlowNetC两种不同网络模型。<br>
　　基本的思路是Encoder-Decoder的思路：It uses an encoder-decoder architecture with additional crosslinks between contracting and expanding network parts,where the encoder computes abstract features from receptive fields of increasing size, and the decoder reestablishes the original resolution via an expanding upconvolutional architecture.FlowNet使用的结构是分为两部分——contracting and expanding networ part的编码器和解码器。其中编码器是用来提取抽象的特征，然后解码器部分对提取出来的特征进一步处理，得到原输入图像分辨率的prediction。
#### contracting part：
- FlowNet:
![/downloads/FlowNet.png](/downloads/FlowNet.png){:height="130" width="300"}
　　在输入端直接将两张图像重叠起来，输入的通道从单张图像的3通道变成6通道，然后通过深度网络，得到feature map。

- FlowNetC:
![/downloads/FlowNetC.png](/downloads/FlowNetC.png){:height="150" width="300"}
　　两张图像是分开进行处理的(分开处理的时候经过的是相同layer)，得到各自的特征图，再找到特征图之间的联系。这里涉及到一个corr层。<br>
　　两个特征分别是f1,f2:w*h*C，然后corr层比较这两个特征图各个块。相关的计算公式如下：
　　其中o表示的是位移量displacement，该公式和卷积的操作是一样的，以x1为中心的patch和以x2为中心的patch，对应位置相乘然后相加，这就是卷积核在图片上的操作，只是卷积网络里是和filter之间进行卷积，且它的weight是不需训练或不可训练。计算这两块之间的联系计算复杂度是 c * K * K，而f1上每个块都要与f2上所有块计算联系，f1上有w * h个块(每个像素点都可以做一个块的中心点)，f2上也有w * h个块，所有整个计算复杂度是c * K * K * ( w * h) * ( w * h)，这里是假设每个块在下一张图上时，可以移动任何位置。但是这样计算复杂度太高了，所以我们可以约束位移范围为d(上下左右移动d)，就是每个块只和它附近D: 2d+1的位置块计算联系，而且以x1为中心的块去和它附近D范围计算联系时，还可以加上步长，就是不必和D范围的每个点都进行计算，可以跳着。

#### expanding part
![/downloads/refinement.png](/downloads/refinement.png){:height="200" width="300"}
　　一边向后unconv，一边直接在小的特征图上直接做预测，然后把结果双线性插值然后连接在unconv后的特征图上，这样的结果重复四次之后，得到的结果分辨率是输入图像的1/4，论文中的说法是这个时候再重复之前的操作已经没有什么必要了，所以可以直接进行双线性插值得到和原始图像相同分辨率的结果。<br>
　　放大部分主要是由上卷积层组成，上卷积层由上采样和一个卷积组成，对feature maps使用upconvolution，并且把它和收缩部分对应的feature map以及一个上采样的预测结果联系起来，每一步都会提升两倍的分辨率。这样做的意义是：This way we preserve both high-level information passed from coarser feature maps and fine local information provided in lower layer feature maps.

### DispNet && DispNetC
　　DispNet是在FlowNet的基础上进行进行了改进，主要的改进是在放大部分，放大部分在每个deconv和前一预测结果的concat后增加一个卷积层，关于这样做的原因是为了使输出的结果图更加平滑，此外放大部分比FlowNet多做了一次deconv，得到的结果分辨率更高，网络的结构如图：
![/downloads/dispnet.png](/downloads/dispnet.png){:height="470" width="400"}
![/downloads/dispnet_explaination.png](/downloads/dispnet_explaination.png){:height="100" width="400"}
　　从上面的网络结构中，可以看到一共存在六个损失函数的，在放大部分使用的损失函数是L1Loss，这时使用到的ground_truth是在开始训练之前经过augmentation的，会有一个downsample的过程，所以对应不同分辨率的预测结果会有相同分辨率的ground_truth与之对应。在训练的时候，并不是一开始所有的loss都会使用到：
![/downloads/loss_1.png](/downloads/loss_1.png){:height="130" width="350"}
![/downloads/loss_2.png](/downloads/loss_2.png){:height="130" width="350"}
　　DispNetC是对应FlowNetC的网络结构的，不同之处在于输入的两张图片经过了两层的卷积操作之后进行相关操作了，而FlowNetC是在三层卷积之后做相关计算，同时将corr操作改为了一维的操作，只在x方向上做运算。
　　