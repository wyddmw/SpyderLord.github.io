---
layout: post
category: dump
title: Batch_Normalization
description: 很早之前看BN时候写的笔记，整理一下上传上来
---

## Batch Normalization的一些备注

　　BN算法是用来解决 Internal Covariate Shift问题的，那么首先我们需要理解什么是Internal Covariate Shift。Batch Normalization是基于Mini-Batch SGD的结构的。对于深度学习这种包含很多隐层的网络结构，在训练的过程中，因为各层参数一直在发生变化，所以每个隐层都面临covariate shift的问题，也就是在训练的过程中，隐层的输入分布不断在变化。这就是所谓的Internal Covariate Shift问题，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是只是发生在输入层。<br>
　　如果输入值X的分布一直在变化，那我们就没有办法稳定的学习规律。然后就提出了BatchNorm的基本思想，能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了Internal Covariate Shift问题了。BN并不是凭空产生的，是有启发来源的，之前的研究中，发现，如果在图像处理中对输入图像进行白化操作的话————就是对输入数据分布变换到0均值，单位方差的正态分布————那么神经网络就会较快速的收敛————对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思就是深度神经网络的每一个隐层都是输入层，不过是对于下一层来说的，那么我们能不能对每个隐层都做白化呢？可以理解为对深层神经网络每个神经元的激活值做简化版的White处理。<br>
　　BN算法可以提高学习的收敛速度。对于BN算法可以参考这个链接：[http://blog.csdn.net/whitesilence/article/details/75667002](http://blog.csdn.net/whitesilence/article/details/75667002)
[http://blog.csdn.net/oppo62258801/article/details/76405196](http://blog.csdn.net/whitesilence/article/details/75667002)<br>
　　BN其实就是把每个隐层神经元的激活输入分布从偏离均值为0方差1的正态分布通过平移压缩或是扩大曲线尖锐程度，调整为均值为0方差为1的标准正态分布。调整到正态分布的作用是什么：意味着在一个标准差的范围内，有64%的概率X的之落在[-1,1]的范围内，两个标准差的范围内，也就是95%的概率X的值落在[-2,2]的范围内。<br>
　　激活值x=WU+B，U是真正的输入，x是某个神经元的激活值，假设非线性函数是sigmoid函数，原始没有经过调整的x的正态分布是均值为-6,方差是-1的，对应的95%的概率落在[-8,-4]之间，对应的非线性的函数值就会明显接近0,这就是典型的梯度饱和区。但是经过BN处理之后，95%的概率落在一段非线性函数接近现行变换的区域，意味着x的小变化会导致非线性函数值较大的变化，也就是梯度变化比较大，也就是梯度非饱和区。所以BN的操作就是把因曾神经元激活输入从不同的正态分布拉回到均值为零，方差为1的正态分布，这样就使得大部分的Activition的之落入到非线性函数的线性区域，其对应的导数远离导数饱和区，这样来加快训练收敛的速度。
在scale和shift操作中，就是相当于对标准正态分布左移或是右移、长胖或是变瘦一点，应该是为了找到非线性和线性的一个平衡点？



 	