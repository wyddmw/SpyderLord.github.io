---
layout: post
category: dump
title: 卷积和滤波之间的区别
description: 如题
---

　　在面试计算所陈老师的时候，陈老师向我提出来的一个问题，虽然之后也有想过，但是一直没有非常系统地解决它，到目前也一直没有真正理解，最后的方式还是借助Google。<br>
　　首先是一个博客链接[卷积和滤波的区别](https://blog.csdn.net/sinat_34546420/article/details/78142735)，有时间一定亲手把这段代码实现一下。<br>
　　目前看到的一个感觉讲的比较明白的博客中是这么讲解这两个问题的：对于同一个kernel来说，滤波操作首先是会自动对边界进行填充处理，然后kernel和其对应的作用域的各个元素相乘然后相加。但是对于卷积操作来说，首先需要对卷积核进行180度的翻转，不会自动进行填充的操作。<br>
　　但是在实际的操作中，对于一个卷积核或是滤波器来说，在进行卷积操作的时候并没有看到会有先将卷积核翻转的操作。<br>
　　所以最直观的一个区别就是滤波操作之后，输出和输入的尺寸是一样的。也正是因为卷积操作中，不会进行自动的填充操作，所以会导致一些边缘信息的丢失，所以在进行卷积操作之前通常也会进行padding操作。

## 吴恩达对这个问题的讲解
　　一个关于卷积操作和互相关的操作。如果我们看的是一本数学教科书，那么卷积的定义是对元素相乘求和。但是如果我们看的是信号的教材的话，在相乘之前需要进行的是需要进行的是将滤波器进行一个镜像的翻转，但是我们在进行定义卷积运算的时候，实际上是跳过了镜像的操作。按照上面的那种操作，我们进行的实际上是互相关的操作。在深度学习中，我们将这个操作称为卷积操作。