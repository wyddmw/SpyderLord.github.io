---
layout: post
category: dump
title: 依然是SVM和线性回归之间的对比问题
description: 遗忘的整理，二分类问题，知乎上的一个回答
---

　　首先是回顾一下线性回归和逻辑回归的问题，二者都是用来对连续值进行预测，称为回归分析，区别于分了问题的离散化的问题分析。逻辑回归是在线性回归的基础上在后面加上一层的sigmoid非线性函数，可以理解为用来对二分类的概率进行预测，输出的结果可以理解为label等于1的概率，二分类可以接上sign（x）符号函数。在之前写到的，softmax分类器可以看做是逻辑回归在多分类问题上的拓展。softmax对输出的解释更加直观好理解一些：对输出进行归一化的处理，将输出解释为对应各个分类的概率，这样处理之后就可以使用交叉熵损失函数了。之前对二元损失函数问题已经整理过一部分了，这里直接给出之前写过博客的链接(二元损失函数问题)[]<br>

## 知乎上的回答：
　　逻辑回归和支持向量机之间的区别：逻辑回归和支持向量机之间的区别只是损失函数不同罢了。逻辑回归的损失函数是Cross Entropy loss交叉熵损失函数，SVM损失函数是折叶损失，两个模型都属于线性分类器，性能相当，区别在于：
- LR的解是受数据本身分布影响的，但是SVM的解不受数据分布的影响。
- LR的输出具有自然的概率意义，但是SVM的输出不具有概率的意义。
- SVM依赖数据表达的距离测度，在建模前需要对数据进行标准化，但是LR不需要。
- SVM受正则化的影响比较大，试验中需要Validation，LR不需要。
- LR适合大样本的学习，SVM适合小样本的学习。
